{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Analysis\n",
    "\n",
    "This notebook presents a comprehensive analysis of air quality data, including advanced feature engineering, predictive modeling, clustering, and anomaly detection techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Interactive visualization with Plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"  # For Jupyter notebook display\n",
    "\n",
    "\n",
    "\n",
    "# Time series analysis\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Machine learning - preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "\n",
    "# Machine learning - regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Machine learning - classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "\n",
    "# System utilities\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Output Directories\n",
    "\n",
    "Setting up directory structure for organizing analysis outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for outputs\n",
    "os.makedirs('Plotly_Plots', exist_ok=True)\n",
    "os.makedirs('Plotly_Plots/data', exist_ok=True)\n",
    "os.makedirs('Plotly_Plots/EDA_plots', exist_ok=True)\n",
    "os.makedirs('Plotly_Plots/preprocessing_plots', exist_ok=True)\n",
    "os.makedirs('Plotly_Plots/correlation_analysis_plots', exist_ok=True)\n",
    "os.makedirs('Plotly_Plots/time_series_analysis_plots', exist_ok=True)\n",
    "os.makedirs('Plotly_Plots/modelling_analysis_plots', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Data Loading and Initial Examination\n",
    "\n",
    "In this phase, we load the air quality dataset and perform initial examination to understand its structure, variables, and potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading the Dataset\n",
    "\n",
    "Loading the Air Quality UCI dataset from Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Phase 1: Data Loading and Initial Examination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "airquality = pd.read_excel('Dataset/AirQualityUCI.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Previewing the Dataset\n",
    "\n",
    "Examining the first few rows to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dataset Dimensions\n",
    "\n",
    "Checking the number of rows and columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {airquality.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Column Names\n",
    "\n",
    "Listing all variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nColumn names:\")\n",
    "print(airquality.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Data Information\n",
    "\n",
    "Examining data types and non-null counts for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Data Types\n",
    "\n",
    "Checking the data type of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData types:\")\n",
    "print(airquality.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Summary Statistics\n",
    "\n",
    "Calculating descriptive statistics for numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Missing Values Analysis\n",
    "\n",
    "Checking for null values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Special Missing Values (-200)\n",
    "\n",
    "Identifying columns with -200 values, which represent missing data in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values (represented as -200)\n",
    "print(\"\\nChecking for -200 values (missing data):\")\n",
    "for col in airquality.columns:\n",
    "    if isinstance(airquality[col].min(), (int, float)) and airquality[col].min() == -200:\n",
    "        print(f\"{col} has -200 values: {(airquality[col] == -200).sum()} ({(airquality[col] == -200).sum()/len(airquality)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Replacing Special Missing Values\n",
    "\n",
    "Converting -200 values to NaN for proper handling of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -200 with NaN\n",
    "airquality = airquality.copy()\n",
    "for col in airquality.columns:\n",
    "    if airquality[col].dtype != 'datetime64[ns]' and airquality[col].dtype != 'object':\n",
    "        airquality[col] = airquality[col].replace(-200, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values (represented as -200)\n",
    "print(\"\\nChecking for -200 values (missing data):\")\n",
    "for col in airquality.columns:\n",
    "    if isinstance(airquality[col].min(), (int, float)) and airquality[col].min() == -200:\n",
    "        print(f\"{col} has -200 values: {(airquality[col] == -200).sum()} ({(airquality[col] == -200).sum()/len(airquality)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 Checking for Duplicates\n",
    "\n",
    "Identifying duplicate rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = airquality.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 Verifying Missing Values\n",
    "\n",
    "Rechecking missing values after replacing -200 with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots: 3 rows, 2 cols. Define specs for different types\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    specs=[[{\"type\": \"domain\", \"colspan\": 2}, None],\n",
    "           [{\"type\": \"xy\"}, {\"type\": \"domain\"}],\n",
    "           [{\"type\": \"xy\", \"colspan\": 2}, None]],\n",
    "    subplot_titles=(\n",
    "        None, # Row 1 title (handled by annotation)\n",
    "        'Missing Data (%)', 'Variable Categories',\n",
    "        'Dataset Time Coverage'\n",
    "    ),\n",
    "    row_heights=[0.2, 0.4, 0.4],\n",
    "    vertical_spacing=0.15\n",
    ")\n",
    "\n",
    "# --- Subplot 1: Dataset Overview (using annotations) ---\n",
    "# --- Subplot 1: Dataset Overview (using annotations) ---\n",
    "fig.add_annotation(\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.5, y=0.95, \n",
    "    text='<b>Air Quality UCI Dataset</b>',\n",
    "    showarrow=False, font=dict(size=20), align='center'\n",
    ")\n",
    "fig.add_annotation(\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.5, y=0.88,\n",
    "    text='Hourly measurements from an Italian city (March 2004 - February 2005)',\n",
    "    showarrow=False, font=dict(size=14), align='center'\n",
    ")\n",
    "fig.add_annotation(\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.5, y=0.81,\n",
    "    text=f'{airquality.shape[0]} hourly records, {airquality.shape[1]} variables',\n",
    "    showarrow=False, font=dict(size=12), align='center'\n",
    ")\n",
    "\n",
    "\n",
    "# --- Subplot 2: Data Completeness (Missing %) --- (Row 2, Col 1)\n",
    "missing_data = airquality.isnull().sum().sort_values(ascending=False)\n",
    "missing_percent = missing_data / len(airquality) * 100\n",
    "missing_df = pd.DataFrame({'Missing Count': missing_data, 'Missing Percent': missing_percent})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Percent') # Sort for horizontal bar\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        y=missing_df.index,\n",
    "        x=missing_df['Missing Percent'],\n",
    "        orientation='h',\n",
    "        name='Missing %',\n",
    "        text=missing_df['Missing Percent'].apply(lambda x: f'{x:.1f}%'),\n",
    "        textposition='outside',\n",
    "        hoverinfo='y+x',\n",
    "        hovertemplate='<b>%{y}</b><br>Missing: %{x:.1f}%<extra></extra>',\n",
    "        marker_color='steelblue'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# --- Subplot 3: Variable Categories (Pie Chart) --- (Row 2, Col 2)\n",
    "categories = ['Air pollutants', 'Sensor readings', 'Environmental']\n",
    "counts = [5, 5, 3]  # CO, NOx, NO2, NMHC, C6H6 | PT08.S1-5 | T, RH, AH\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99'] # Keep original colors\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=categories,\n",
    "        values=counts,\n",
    "        name='Categories',\n",
    "        marker_colors=colors,\n",
    "        hole=0.3, \n",
    "        hoverinfo='label+percent+value',\n",
    "        textinfo='percent+label',\n",
    "        insidetextorientation='radial'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# --- Subplot 4: Time Span Visualization --- (Row 3, Col 1, colspan=2)\n",
    "date_range = pd.to_datetime(airquality['Date']).unique() # Get unique dates\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=date_range,\n",
    "        y=np.ones(len(date_range)),\n",
    "        mode='markers',\n",
    "        marker=dict(symbol='line-ns-open', size=8, color='blue'), \n",
    "        name='Data Points',\n",
    "        hoverinfo='x',\n",
    "        hovertemplate='%{x|%Y-%m-%d}<extra></extra>'\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "fig.update_yaxes(showticklabels=False, showgrid=False, zeroline=False, row=3, col=1)\n",
    "fig.update_xaxes(title_text='Date', row=3, col=1, gridcolor='lightgrey', gridwidth=1)\n",
    "\n",
    "# --- Layout Updates ---\n",
    "fig.update_layout(\n",
    "    title_text='Air Quality Dataset Overview', title_x=0.5,\n",
    "    height=800, \n",
    "    showlegend=False,\n",
    "    # Add source text via annotation below plot area\n",
    "    annotations=list(fig.layout.annotations) + [\n",
    "        go.layout.Annotation(\n",
    "            text=\"Source: UCI Machine Learning Repository | Savona, Italy - De Vito et al. (2008) | Features include ground truth measurements (GT) and sensor responses\",\n",
    "            showarrow=False,\n",
    "            xref='paper', yref='paper',\n",
    "            x=0.5, y=-0.12, # Adjust y position to be below plot\n",
    "            xanchor='center', yanchor='top',\n",
    "            font=dict(size=8.5)\n",
    "        )\n",
    "    ],\n",
    "    margin=dict(l=50, r=50, t=100, b=120) # Increase bottom margin for source text\n",
    ")\n",
    "\n",
    "# Update subplot titles font size\n",
    "for annotation in fig.layout.annotations:\n",
    "    # Check if it's a subplot title generated by make_subplots\n",
    "    if annotation.text in ['Missing Data (%)', 'Variable Categories', 'Dataset Time Coverage']:\n",
    "        annotation.font.size = 14\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/data/dataset_description_plotly.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this phase, we perform a deeper investigation of the dataset through visualizations and statistical analyses to understand distributions, patterns, and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 Summary\n",
    "\n",
    "In this initial phase, we successfully loaded the air quality dataset and performed a preliminary examination to understand its structure, variables, and potential issues. The dataset contains hourly measurements of various pollutants and environmental factors over a one-year period, providing a rich source of information for analysis.\n",
    "\n",
    "Key findings from this phase include:\n",
    "\n",
    "- The dataset contains 9,357 hourly records with 13 variables, including pollutant concentrations, temperature, and humidity measurements.\n",
    "- Several variables have missing values, particularly NMHC(GT) with approximately 90% missing data.\n",
    "- Missing values are represented as -200 in the original dataset and have been converted to NaN for proper handling.\n",
    "- No duplicate rows were found in the dataset.\n",
    "- The dataset includes both ground truth measurements (GT) and sensor responses (PT08.S1 to PT08.S5).\n",
    "\n",
    "This initial examination provides the foundation for the more detailed analyses in subsequent phases. The identified data quality issues, particularly the missing values, will need to be addressed in the preprocessing phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Missing Values Visualization\n",
    "\n",
    "Creating a heatmap to visualize the pattern of missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPhase 2: Exploratory Data Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = airquality.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure df_clean is defined and pandas (pd) is imported\n",
    "# df_clean = airquality.copy() # If needed\n",
    "\n",
    "# --- Plotly Missing Values Bar Chart ---\n",
    "missing_percent = df_clean.isna().mean().sort_values(ascending=False) * 100\n",
    "missing_percent_df = missing_percent[missing_percent > 0].reset_index()\n",
    "missing_percent_df.columns = ['Column', 'Percentage']\n",
    "\n",
    "fig = px.bar(\n",
    "    missing_percent_df,\n",
    "    x='Column',\n",
    "    y='Percentage',\n",
    "    title='Percentage of Missing Values by Column',\n",
    "    labels={'Percentage': 'Missing Values (%)', 'Column': 'Columns'},\n",
    "    text=missing_percent_df['Percentage'].apply(lambda x: f'{x:.1f}%'),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    textposition='outside',\n",
    "    marker_color='steelblue',\n",
    "    hovertemplate='<b>%{x}</b><br>Missing: %{y:.1f}%<extra></extra>'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45,\n",
    "    yaxis_gridcolor='lightgrey',\n",
    "    yaxis_gridwidth=1,\n",
    "    bargap=0.2,\n",
    "    yaxis_title='Missing Values (%)', # Ensure y-axis label is set\n",
    "    xaxis_title='Columns' # Ensure x-axis label is set\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/EDA_plots/dataset_description_plotly.png', scale=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Distribution Analysis\n",
    "\n",
    "Examining the distribution of key variables using histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure df_clean, np, pd are defined/imported\n",
    "# df_clean = airquality.copy() # If needed\n",
    "\n",
    "# --- Plotly Distribution Histograms ---\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Create subplots\n",
    "n_cols = 3\n",
    "n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=numeric_cols)\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    row = (i // n_cols) + 1\n",
    "    col_num = (i % n_cols) + 1\n",
    "    # Use go.Histogram for subplots\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=df_clean[col].dropna(),\n",
    "            name=col,\n",
    "            showlegend=False,\n",
    "            hovertemplate=f'<b>{col}</b><br>Value Range: %{{x}}<br>Count: %{{y}}<extra></extra>'\n",
    "            # histnorm='probability density' # Add if KDE was important\n",
    "        ),\n",
    "        row=row, col=col_num\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Distribution of Numeric Variables',\n",
    "    height=n_rows * 300, \n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Update subplot titles font size\n",
    "for annotation in fig.layout.annotations:\n",
    "    annotation.font.size = 12\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/EDA_plots/histograms_plotly.png', scale=1.5, width=1000, height=n_rows*300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Outlier Analysis\n",
    "\n",
    "Identifying outliers in key variables using box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure df_clean, numeric_cols, pd are defined/imported\n",
    "# df_clean = airquality.copy() # If needed\n",
    "# numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# --- Plotly Outlier Box Plots ---\n",
    "# Melt the dataframe for Plotly Express\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist() # Ensure numeric_cols is defined here\n",
    "df_clean_melt = pd.melt(df_clean[numeric_cols], var_name='variable', value_name='value')\n",
    "\n",
    "fig = px.box(\n",
    "    df_clean_melt,\n",
    "    x='variable',\n",
    "    y='value',\n",
    "    title='Box Plots of Numeric Variables',\n",
    "    points='outliers', \n",
    "    height=600 \n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=None, \n",
    "    yaxis_title='Value',\n",
    "    xaxis_tickangle=-90 \n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    hovertemplate='<b>%{x}</b><br>Value: %{y}<br>Variable: %{x}<extra></extra>', # Improved hover\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/EDA_plots/boxplots_plotly.png', scale=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Temporal Patterns\n",
    "\n",
    "Analyzing how variables change over time to identify trends and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series plots for key pollutants\n",
    "# First, ensure datetime format\n",
    "df_clean['DateTime'] = pd.to_datetime(df_clean['Date'].astype(str) + ' ' + df_clean['Time'].astype(str))\n",
    "df_clean = df_clean.set_index('DateTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure df_clean is defined, has DateTime index, and pd, go are imported\n",
    "# df_clean = airquality.copy()\n",
    "# df_clean['DateTime'] = pd.to_datetime(df_clean['Date'].astype(str) + ' ' + df_clean['Time'].astype(str))\n",
    "# df_clean = df_clean.set_index('DateTime') # Ensure this is done before this cell\n",
    "\n",
    "# --- Plotly Pollutant Time Series ---\n",
    "pollutants = ['CO(GT)', 'NOx(GT)', 'NO2(GT)']\n",
    "\n",
    "# Resample data first\n",
    "df_daily_mean = df_clean[pollutants].resample('D').mean()\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=len(pollutants), cols=1, shared_xaxes=True,\n",
    "                    subplot_titles=[f'Daily Average {p}' for p in pollutants])\n",
    "\n",
    "for i, pollutant in enumerate(pollutants):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_daily_mean.index,\n",
    "            y=df_daily_mean[pollutant],\n",
    "            mode='lines',\n",
    "            name=pollutant,\n",
    "            showlegend=False,\n",
    "            hovertemplate=f'<b>{pollutant}</b><br>Date: %{{x|%Y-%m-%d}}<br>Concentration: %{{y:.2f}}<extra></extra>'\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "    fig.update_yaxes(title_text='Concentration', row=i + 1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Daily Average Pollutant Concentrations',\n",
    "    height=300 * len(pollutants),\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "# Update subplot titles font size\n",
    "for annotation in fig.layout.annotations:\n",
    "     if annotation.text in [f'Daily Average {p}' for p in pollutants]:\n",
    "        annotation.font.size = 14\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/EDA_plots/time_series_pollutants_plotly.png', scale=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure df_clean is defined, has DateTime index, and pd, go are imported\n",
    "# df_clean = airquality.copy()\n",
    "# df_clean['DateTime'] = pd.to_datetime(df_clean['Date'].astype(str) + ' ' + df_clean['Time'].astype(str))\n",
    "# df_clean = df_clean.set_index('DateTime') # Ensure this is done before this cell\n",
    "\n",
    "# --- Plotly Environmental Variable Time Series ---\n",
    "env_vars = ['T', 'RH', 'AH']\n",
    "\n",
    "# Resample data first\n",
    "df_env_daily_mean = df_clean[env_vars].resample('D').mean()\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=len(env_vars), cols=1, shared_xaxes=True,\n",
    "                    subplot_titles=[f'Daily Average {v}' for v in env_vars])\n",
    "\n",
    "for i, var in enumerate(env_vars):\n",
    "    if var == 'T': ylabel = 'Temperature (°C)'\n",
    "    elif var == 'RH': ylabel = 'Relative Humidity (%)'\n",
    "    elif var == 'AH': ylabel = 'Absolute Humidity (g/m³)'\n",
    "    else: ylabel = 'Value'\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_env_daily_mean.index,\n",
    "            y=df_env_daily_mean[var],\n",
    "            mode='lines',\n",
    "            name=var,\n",
    "            showlegend=False,\n",
    "            hovertemplate=f'<b>{var}</b><br>Date: %{{x|%Y-%m-%d}}<br>{ylabel}: %{{y:.2f}}<extra></extra>'\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "    fig.update_yaxes(title_text=ylabel, row=i + 1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Daily Average Environmental Variables',\n",
    "    height=300 * len(env_vars),\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "# Update subplot titles font size\n",
    "for annotation in fig.layout.annotations:\n",
    "    if annotation.text in [f'Daily Average {v}' for v in env_vars]: # Check if it's a subplot title\n",
    "        annotation.font.size = 14\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/EDA_plots/time_series_env_plotly.png', scale=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure df_clean, pd, np, go, make_subplots are defined/imported\n",
    "# df_clean = airquality.copy() # If needed\n",
    "\n",
    "# --- Plotly Pair Plot Approximation ---\n",
    "key_vars = ['CO(GT)', 'NOx(GT)', 'NO2(GT)', 'T', 'RH']\n",
    "# Sample data and drop NA for plotting\n",
    "df_sample = df_clean[key_vars].dropna().sample(n=min(1000, len(df_clean.dropna())), random_state=42) # Sample safely\n",
    "\n",
    "n_vars = len(key_vars)\n",
    "fig = make_subplots(rows=n_vars, cols=n_vars, \n",
    "                    # shared_xaxes=True, shared_yaxes=True, # Causes issues with histograms on diag\n",
    "                    horizontal_spacing=0.03, vertical_spacing=0.03)\n",
    "\n",
    "for i in range(n_vars):\n",
    "    for j in range(n_vars):\n",
    "        row = i + 1\n",
    "        col = j + 1\n",
    "        x_var = key_vars[j]\n",
    "        y_var = key_vars[i]\n",
    "\n",
    "        if i == j: # Diagonal: Histogram\n",
    "            fig.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=df_sample[x_var],\n",
    "                    name=x_var,\n",
    "                    showlegend=False,\n",
    "                    nbinsx=30,\n",
    "                    marker_color='grey',\n",
    "                    hovertemplate=f'<b>{x_var}</b><br>Range: %{{x}}<br>Count: %{{y}}<extra></extra>'\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "        else: # Off-diagonal: Scatter plot\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_sample[x_var],\n",
    "                    y=df_sample[y_var],\n",
    "                    mode='markers',\n",
    "                    name='',\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, opacity=0.6, color='blue'),\n",
    "                    hovertemplate=f'<b>{x_var}</b>: %{{x:.2f}}<br><b>{y_var}</b>: %{{y:.2f}}<extra></extra>'\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "\n",
    "        # Add axis labels only to the bottom row and leftmost column\n",
    "        if i == n_vars - 1:\n",
    "            fig.update_xaxes(title_text=x_var, row=row, col=col, title_font=dict(size=10), tickfont=dict(size=8))\n",
    "        else:\n",
    "             fig.update_xaxes(showticklabels=False, row=row, col=col) # Hide ticks for inner plots\n",
    "\n",
    "        if j == 0:\n",
    "            fig.update_yaxes(title_text=y_var, row=row, col=col, title_font=dict(size=10), tickfont=dict(size=8))\n",
    "        else:\n",
    "             fig.update_yaxes(showticklabels=False, row=row, col=col) # Hide ticks for inner plots\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Pair Plot Approximation of Key Variables (Sampled Data)',\n",
    "    height=800, width=800,\n",
    "    hovermode='closest',\n",
    "    bargap=0.01, # For histograms\n",
    "    showlegend=False # Ensure no legend items appear\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/EDA_plots/pairplot_plotly.png', scale=1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Summary: Key Findings and Observations\n",
    "\n",
    "### Data Description and Patterns\n",
    "- The dataset contains 9,357 hourly records of air quality and meteorological variables from an Italian city.\n",
    "- Key variables include concentrations of CO, NOx, NO2, C6H6, and sensor responses, as well as temperature (T), relative humidity (RH), and absolute humidity (AH).\n",
    "- There are significant missing values in some variables, especially NMHC(GT) (~90% missing), and moderate missingness in CO(GT), NOx(GT), and NO2(GT) (~18%).\n",
    "- The summary statistics show a wide range of values for pollutants, with some variables (e.g., CO(GT), NOx(GT)) having outliers and skewed distributions.\n",
    "\n",
    "### Visual Patterns and Anomalies\n",
    "- Histograms reveal that many pollutant concentrations are right-skewed, with a majority of values clustered at the lower end and a long tail of higher values.\n",
    "- Box plots confirm the presence of outliers, especially for CO(GT), NOx(GT), and C6H6(GT).\n",
    "- Time series plots show clear daily and seasonal trends in pollutant concentrations and meteorological variables. For example, CO and NOx levels tend to be higher in colder months.\n",
    "- Pair plots (scatter plots) indicate positive correlations between some pollutants (e.g., CO and NOx), and relationships between temperature/humidity and pollutant levels.\n",
    "\n",
    "### Interesting Observations\n",
    "- The high proportion of missing data in NMHC(GT) may require imputation or exclusion from some analyses.\n",
    "- Outliers and non-normal distributions suggest the need for robust statistical methods or data transformation in further modeling.\n",
    "- The data's temporal structure (hourly, with date and time) enables time series analysis and investigation of diurnal/seasonal cycles.\n",
    "\n",
    "### Next Steps\n",
    "- Address missing values and outliers in preprocessing.\n",
    "- Explore feature engineering and correlation analysis for predictive modeling.\n",
    "- Consider stratified or time-based data splitting for model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exploratory data analysis completed. Visualizations saved to exploratory_data_analysis/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2 Summary\n",
    "\n",
    "The exploratory data analysis phase provided valuable insights into the distribution, patterns, and relationships within the air quality dataset. Through various visualizations and statistical analyses, we gained a deeper understanding of the data characteristics and identified important features for further investigation.\n",
    "\n",
    "Key findings from this phase include:\n",
    "\n",
    "- Pollutant concentrations (CO, NOx, NO2, C6H6) showed right-skewed distributions, with most values clustered at the lower end and a long tail of higher values.\n",
    "- Temperature (T) followed a bimodal distribution, reflecting seasonal variations throughout the year.\n",
    "- Clear temporal patterns were observed in pollution levels, with distinct daily, weekly, and seasonal variations.\n",
    "- Strong correlations were found between related pollutants (e.g., NOx and NO2), and between pollutants and their corresponding sensor readings.\n",
    "- Environmental factors like temperature and humidity showed significant relationships with pollution levels, with lower temperatures often associated with higher pollution concentrations.\n",
    "\n",
    "These insights inform our approach to data preprocessing, feature engineering, and modeling in subsequent phases. The identified patterns and relationships will guide our selection of relevant features and appropriate modeling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Cleaning\n",
    "\n",
    "Handling missing values, removing outliers, and preparing data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Phase 3.1: Initial Overview, Duplicates, and Missing Values ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial overview\n",
    "print(\"Initial Dataset Overview:\")\n",
    "print(f\"Number of observations: {airquality.shape[0]}\")\n",
    "print(f\"Number of variables: {airquality.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and remove duplicates\n",
    "duplicates = airquality.duplicated().sum()\n",
    "print(f\"\\nDuplicate Records: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"Removing duplicate records...\")\n",
    "    airquality = airquality.drop_duplicates()\n",
    "    print(f\"Dataset shape after removing duplicates: {airquality.shape}\")\n",
    "else:\n",
    "    print(\"No duplicate records found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Engineering\n",
    "\n",
    "Creating new features to enhance analysis and modeling capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle -200 as missing values\n",
    "airquality_clean = airquality.copy()\n",
    "print(\"\\nMissing Values Before Treatment:\")\n",
    "for col in airquality_clean.columns:\n",
    "    if airquality_clean[col].dtype != 'datetime64[ns]' and airquality_clean[col].dtype != 'object':\n",
    "        mask = airquality_clean[col] == -200\n",
    "        missing_count = mask.sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"{col}: {missing_count} missing values ({missing_count/len(airquality_clean)*100:.2f}%)\")\n",
    "            airquality_clean.loc[mask, col] = np.nan\n",
    "\n",
    "print(\"\\nMissing Values Treatment Strategy:\")\n",
    "for col in airquality_clean.columns:\n",
    "    if col not in ['Date', 'Time'] and airquality_clean[col].isna().sum() > 0:\n",
    "        missing_pct = airquality_clean[col].isna().sum() / len(airquality_clean) * 100\n",
    "        if missing_pct > 80:\n",
    "            print(f\"{col}: {missing_pct:.2f}% missing - Column will be dropped\")\n",
    "        elif missing_pct > 30:\n",
    "            print(f\"{col}: {missing_pct:.2f}% missing - Sensor correlations will be used for imputation\")\n",
    "        else:\n",
    "            print(f\"{col}: {missing_pct:.2f}% missing - Forward fill with rolling mean\")\n",
    "\n",
    "# Drop high-missing column\n",
    "if 'NMHC(GT)' in airquality_clean.columns and airquality_clean['NMHC(GT)'].isna().sum() / len(airquality_clean) > 0.8:\n",
    "    print(\"Dropping NMHC(GT) due to excessive missing values\")\n",
    "    airquality_clean = airquality_clean.drop(columns=['NMHC(GT)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Phase 3.2: Imputation and Outlier Handling ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set datetime index\n",
    "airquality_clean['DateTime'] = pd.to_datetime(airquality_clean['Date'].astype(str) + ' ' + airquality_clean['Time'].astype(str))\n",
    "airquality_clean = airquality_clean.set_index('DateTime').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor-based imputation\n",
    "pollutant_sensor_pairs = [('CO(GT)', 'PT08.S1(CO)'), ('NOx(GT)', 'PT08.S3(NOx)'), ('NO2(GT)', 'PT08.S4(NO2)')]\n",
    "for pollutant, sensor in pollutant_sensor_pairs:\n",
    "    if pollutant in airquality_clean.columns and sensor in airquality_clean.columns:\n",
    "        if airquality_clean[pollutant].isna().sum() > 0:\n",
    "            valid_data = airquality_clean[[pollutant, sensor]].dropna()\n",
    "            if len(valid_data) > 0:\n",
    "                correlation = valid_data[pollutant].corr(valid_data[sensor])\n",
    "                print(f\"Correlation between {pollutant} and {sensor}: {correlation:.4f}\")\n",
    "                if abs(correlation) > 0.5:\n",
    "                    model = LinearRegression()\n",
    "                    model.fit(valid_data[[sensor]], valid_data[pollutant])\n",
    "                    predict_indices = airquality_clean[pollutant].isna() & ~airquality_clean[sensor].isna()\n",
    "                    airquality_clean.loc[predict_indices, pollutant] = model.predict(airquality_clean.loc[predict_indices, [sensor]])\n",
    "                    print(f\"Used regression model to impute {predict_indices.sum()} values in {pollutant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Temporal Feature Engineering\n",
    "\n",
    "Extracting time-based features from datetime information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling mean and fill\n",
    "for col in airquality_clean.columns:\n",
    "    if col not in ['Date', 'Time'] and airquality_clean[col].isna().sum() > 0:\n",
    "        missing_before = airquality_clean[col].isna().sum()\n",
    "        rolling_mean = airquality_clean[col].rolling(window=24, min_periods=1).mean()\n",
    "        airquality_clean[col] = airquality_clean[col].fillna(rolling_mean)\n",
    "        if airquality_clean[col].isna().sum() > 0:\n",
    "            airquality_clean[col] = airquality_clean[col].ffill()\n",
    "        if airquality_clean[col].isna().sum() > 0:\n",
    "            airquality_clean[col] = airquality_clean[col].bfill()\n",
    "        if airquality_clean[col].isna().sum() > 0:\n",
    "            airquality_clean[col] = airquality_clean[col].fillna(airquality_clean[col].mean())\n",
    "        print(f\"{col}: Imputed {missing_before} missing values\")\n",
    "\n",
    "print(f\"\\nRemaining missing values: {airquality_clean.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier handling\n",
    "numeric_cols = airquality_clean.select_dtypes(include=['float64', 'int64']).columns\n",
    "numeric_cols = [col for col in numeric_cols if col not in ['Date', 'Time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a melted dataframe for visualization\n",
    "df_melted = pd.melt(airquality_clean.reset_index()[numeric_cols], var_name='variable', value_name='value')\n",
    "\n",
    "# Create boxplot\n",
    "fig = px.box(\n",
    "    df_melted,\n",
    "    x='variable',\n",
    "    y='value',\n",
    "    title='Box Plots Before Outlier Treatment',\n",
    "    labels={'variable': 'Variables', 'value': 'Values'},\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45,\n",
    "    xaxis_title=None,\n",
    "    yaxis_title='Values',\n",
    "    margin=dict(l=50, r=50, t=80, b=100),\n",
    "    template='plotly_white'  # Clean white background\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/preprocessing_plots/before_outlier_treatment_boxplot_plotly.png', scale=1.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and cap outliers\n",
    "for col in numeric_cols:\n",
    "    Q1 = airquality_clean[col].quantile(0.25)\n",
    "    Q3 = airquality_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outliers = ((airquality_clean[col] < lower) | (airquality_clean[col] > upper)).sum()\n",
    "    print(f\"{col}: {outliers} outliers detected\")\n",
    "    if outliers > 0:\n",
    "        airquality_clean[col] = airquality_clean[col].clip(lower, upper)\n",
    "        print(f\"  - Outliers capped between {lower:.2f} and {upper:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a melted dataframe for visualization\n",
    "df_melted = pd.melt(airquality_clean.reset_index()[numeric_cols], var_name='variable', value_name='value')\n",
    "\n",
    "# Create boxplot\n",
    "fig = px.box(\n",
    "    df_melted,\n",
    "    x='variable',\n",
    "    y='value',\n",
    "    title='Box Plots Before Outlier Treatment',\n",
    "    labels={'variable': 'Variables', 'value': 'Values'},\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45,\n",
    "    xaxis_title=None,\n",
    "    yaxis_title='Values',\n",
    "    margin=dict(l=50, r=50, t=80, b=100),\n",
    "    template='plotly_white'  # Clean white background\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/preprocessing_plots/after_outlier_treatment_boxplot_plotly.png', scale=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Phase 3.3: Data Transformation and Finalization ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "print(\"Standardizing numeric features...\")\n",
    "airquality_standardized = airquality_clean.copy()\n",
    "scaler = StandardScaler()\n",
    "for col in numeric_cols:\n",
    "    airquality_standardized[col] = scaler.fit_transform(airquality_standardized[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed and standardized data\n",
    "airquality_clean.to_csv('preprocessing/preprocessed_data.csv')\n",
    "airquality_standardized.to_csv('preprocessing/standardized_data.csv')\n",
    "\n",
    "print(f\"\\nFinal Preprocessed Dataset Shape: {airquality_clean.shape}\")\n",
    "print(f\"Columns: {list(airquality_clean.columns)}\")\n",
    "print(\"Preprocessed data saved to 'preprocessing/preprocessed_data.csv'\")\n",
    "print(\"Standardized data saved to 'preprocessing/standardized_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data preprocessing completed. Results saved to preprocessing/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3.4: Feature Engineering & Advanced Preprocessing\n",
    "\n",
    "In this section, we will create new features to enhance our analysis and modeling capabilities. These engineered features will help capture temporal patterns, environmental conditions, and other factors that may influence air quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Phase 3.4: Feature Engineering & Advanced Preprocessing ---\")\n",
    "\n",
    "# Load the preprocessed data\n",
    "df = pd.read_csv('preprocessing/preprocessed_data.csv', index_col=0, parse_dates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rush Hour Indicator\n",
    "\n",
    "Creating a feature to indicate if a timestamp falls within typical rush hours (7-9 AM, 5-8 PM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour from the datetime index\n",
    "df['hour'] = df.index.hour\n",
    "\n",
    "# Create rush hour indicator\n",
    "df['is_rush_hour'] = ((df['hour'] >= 7) & (df['hour'] <= 9)) | ((df['hour'] >= 17) & (df['hour'] <= 20))\n",
    "df['is_rush_hour'] = df['is_rush_hour'].astype(int)  # Convert boolean to 0/1\n",
    "\n",
    "# Visualize pollution levels during rush hours vs. non-rush hours using plotly\n",
    "pollutants = ['CO(GT)', 'NOx(GT)', 'NO2(GT)']\n",
    "fig = make_subplots(rows=len(pollutants), cols=1, \n",
    "                    subplot_titles=[f'{p} Levels: Rush Hour vs. Non-Rush Hour' for p in pollutants],\n",
    "                    vertical_spacing=0.1)\n",
    "\n",
    "for i, pollutant in enumerate(pollutants):\n",
    "    # Create box plots for rush hour (1)\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=df[df['is_rush_hour']==1]['is_rush_hour'],\n",
    "            y=df[df['is_rush_hour']==1][pollutant],\n",
    "            name=\"Rush Hour\",\n",
    "            boxmean=True,  # adds a marker for the mean\n",
    "            marker_color='darkblue',\n",
    "            hovertemplate=f\"{pollutant}: %{{y:.2f}}<extra></extra>\"\n",
    "        ),\n",
    "        row=i+1, col=1\n",
    "    )\n",
    "    \n",
    "    # Create box plots for non-rush hour (0)\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=df[df['is_rush_hour']==0]['is_rush_hour'],\n",
    "            y=df[df['is_rush_hour']==0][pollutant],\n",
    "            name=\"Non-Rush Hour\",\n",
    "            boxmean=True,\n",
    "            marker_color='lightblue',\n",
    "            hovertemplate=f\"{pollutant}: %{{y:.2f}}<extra></extra>\"\n",
    "        ),\n",
    "        row=i+1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(\n",
    "        ticktext=[\"Non-Rush Hour\", \"Rush Hour\"], \n",
    "        tickvals=[0, 1],\n",
    "        title_text=\"Time Period\",\n",
    "        row=i+1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(\n",
    "        title_text=\"Concentration\",\n",
    "        row=i+1, col=1\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    width=800,\n",
    "    title_text=\"Pollution Levels: Rush Hour vs. Non-Rush Hour\",\n",
    "    showlegend=i==0  # Only show legend for the first pollutant\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/preprocessing_plots/rush_hour_boxplot_plotly.png', scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for Rush Hour Indicator\n",
    "\n",
    "The rush hour indicator is valuable for several reasons:\n",
    "\n",
    "1. **Traffic Patterns**: Rush hours typically coincide with increased traffic volume, which is a major source of urban air pollution.\n",
    "2. **Predictive Power**: This feature can help models identify and predict pollution spikes associated with commuting patterns.\n",
    "3. **Policy Relevance**: Understanding pollution patterns during rush hours can inform traffic management policies and public health advisories.\n",
    "4. **Temporal Context**: It provides important temporal context that raw timestamp data doesn't explicitly capture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekend vs. Weekday Feature\n",
    "\n",
    "Creating a binary feature to distinguish between weekdays and weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract day of week (0=Monday, 6=Sunday)\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "\n",
    "# Create weekend indicator (5=Saturday, 6=Sunday)\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Visualize pollution levels on weekends vs. weekdays using plotly\n",
    "fig = make_subplots(rows=3, cols=1, \n",
    "                    subplot_titles=[f'{pollutant} Levels: Weekend vs. Weekday' for pollutant in ['CO(GT)', 'NOx(GT)', 'NO2(GT)']])\n",
    "\n",
    "for i, pollutant in enumerate(['CO(GT)', 'NOx(GT)', 'NO2(GT)']):\n",
    "    # Create box plots for weekend (1)\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=df[df['is_weekend']==1]['is_weekend'],\n",
    "            y=df[df['is_weekend']==1][pollutant],\n",
    "            name=\"Weekend\",\n",
    "            boxmean=True,\n",
    "            marker_color='darkgreen',\n",
    "            hovertemplate=f\"{pollutant}: %{{y:.2f}}<extra></extra>\"\n",
    "        ),\n",
    "        row=i+1, col=1\n",
    "    )\n",
    "    \n",
    "    # Create box plots for weekday (0)\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=df[df['is_weekend']==0]['is_weekend'],\n",
    "            y=df[df['is_weekend']==0][pollutant],\n",
    "            name=\"Weekday\",\n",
    "            boxmean=True,\n",
    "            marker_color='lightgreen',\n",
    "            hovertemplate=f\"{pollutant}: %{{y:.2f}}<extra></extra>\"\n",
    "        ),\n",
    "        row=i+1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(\n",
    "        ticktext=[\"Weekday\", \"Weekend\"], \n",
    "        tickvals=[0, 1],\n",
    "        title_text=\"Day Type\",\n",
    "        row=i+1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(\n",
    "        title_text=\"Concentration\",\n",
    "        row=i+1, col=1\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    width=800,\n",
    "    title_text=\"Pollution Levels: Weekend vs. Weekday\",\n",
    "    showlegend=i==0  # Only show legend for the first subplot\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/preprocessing_plots/weekend_boxplot_plotly.png', scale=1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for Weekend vs. Weekday Feature\n",
    "\n",
    "The weekend/weekday distinction is important for these reasons:\n",
    "\n",
    "1. **Activity Patterns**: Human activity patterns differ significantly between weekdays and weekends, affecting emission sources.\n",
    "2. **Industrial Operations**: Many industrial facilities operate on different schedules during weekends.\n",
    "3. **Traffic Volumes**: Traffic patterns and volumes typically differ between weekdays and weekends.\n",
    "4. **Model Accuracy**: Including this feature can help models account for weekly cyclical patterns in pollution levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Season Feature\n",
    "\n",
    "Creating a categorical feature to represent seasons, which can significantly affect pollution patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract month\n",
    "df['month'] = df.index.month\n",
    "\n",
    "# Create season feature (Northern Hemisphere)\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:  # 9, 10, 11\n",
    "        return 'Fall'\n",
    "\n",
    "df['season'] = df['month'].apply(get_season)\n",
    "\n",
    "# Visualize pollution levels by season\n",
    "pollutants = ['CO(GT)', 'NOx(GT)', 'NO2(GT)']\n",
    "fig = go.Figure()\n",
    "\n",
    "# Create subplots - one row per pollutant\n",
    "fig = make_subplots(\n",
    "    rows=len(pollutants),\n",
    "    cols=1,\n",
    "    subplot_titles=[f'{p} Levels by Season' for p in pollutants],\n",
    "    vertical_spacing=0.1\n",
    ")\n",
    "\n",
    "seasons = ['Winter', 'Spring', 'Summer', 'Fall']\n",
    "colors = ['#A1D6E2', '#81B29A', '#F2CC8F', '#E07A5F']\n",
    "\n",
    "for i, pollutant in enumerate(pollutants):\n",
    "    for j, season in enumerate(seasons):\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=df[df['season'] == season][pollutant],\n",
    "                name=season,\n",
    "                marker_color=colors[j],\n",
    "                showlegend=i==0,  # Only show legend for the first pollutant\n",
    "                hovertemplate=f\"{pollutant} ({season}): %{{y:.2f}}<extra></extra>\"\n",
    "            ),\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "    \n",
    "    fig.update_yaxes(title_text='Concentration', row=i+1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    boxmode='group',\n",
    "    title_text='Pollution Levels by Season',\n",
    "    legend_title_text='Season'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/preprocessing_plots/season_boxplot_plotly.png', scale=1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for Season Feature\n",
    "\n",
    "The season feature is valuable for these reasons:\n",
    "\n",
    "1. **Meteorological Conditions**: Seasons bring different weather patterns that affect pollution dispersion and chemistry.\n",
    "2. **Emission Sources**: Seasonal activities like heating in winter or increased air conditioning in summer affect emissions.\n",
    "3. **Photochemical Reactions**: Seasonal variations in sunlight affect photochemical reactions that create secondary pollutants.\n",
    "4. **Long-term Patterns**: This feature helps models capture long-term cyclical patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Based Features\n",
    "\n",
    "Creating additional time-based features to capture temporal patterns at different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour of day as cyclical features using sine and cosine transformations\n",
    "# This preserves the cyclical nature of time (hour 23 is close to hour 0)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "\n",
    "# Day of week as cyclical features\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['day_of_week']/7)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['day_of_week']/7)\n",
    "\n",
    "# Month as cyclical features\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "\n",
    "# Create subplots with plotly\n",
    "fig = make_subplots(rows=2, cols=1, \n",
    "                    subplot_titles=[\"Average CO(GT) by Hour of Day\", \n",
    "                                   \"Cyclical Representation of Hours\"])\n",
    "\n",
    "# Average CO by hour plot\n",
    "hourly_co = df.groupby('hour')['CO(GT)'].mean()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=hourly_co.index, \n",
    "        y=hourly_co.values,\n",
    "        mode='lines+markers',\n",
    "        name='CO(GT)',\n",
    "        line=dict(color='blue'),\n",
    "        hovertemplate='Hour: %{x}<br>CO(GT): %{y:.2f}<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Cyclical representation plot\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df['hour_sin'],\n",
    "        y=df['hour_cos'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=8,\n",
    "            color=df['hour'],\n",
    "            colorscale='Viridis',\n",
    "            colorbar=dict(\n",
    "                title='Hour of Day',\n",
    "                x=1,\n",
    "                y=0.5,\n",
    "                yanchor='middle'\n",
    "            ),\n",
    "            showscale=True\n",
    "        ),\n",
    "        hovertemplate='Hour: %{marker.color:.0f}<br>sin(hour): %{x:.2f}<br>cos(hour): %{y:.2f}<extra></extra>'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    width=900,\n",
    "    title_text='Cyclical Time Features Representation',\n",
    "    showlegend=False,\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_xaxes(title_text='Hour of Day', row=1, col=1)\n",
    "fig.update_yaxes(title_text='CO(GT) Concentration', row=1, col=1)\n",
    "fig.update_xaxes(title_text='sin(hour)', row=2, col=1)\n",
    "fig.update_yaxes(title_text='cos(hour)', row=2, col=1)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/preprocessing_plots/cyclical_time_features_plotly.png', scale=1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for Cyclical Time Features\n",
    "\n",
    "Cyclical time features offer several advantages:\n",
    "\n",
    "1. **Preserving Cyclical Nature**: They maintain the cyclical relationship between time periods (e.g., hour 23 is close to hour 0).\n",
    "2. **Model Compatibility**: These transformations make time features more suitable for machine learning models.\n",
    "3. **Capturing Periodicity**: They help models identify periodic patterns at different time scales (daily, weekly, monthly).\n",
    "4. **Feature Importance**: They often provide more predictive power than raw time values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag Features\n",
    "\n",
    "Creating lag features to capture the relationship between current and past pollution levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create lag features for key pollutants\n",
    "for pollutant in ['CO(GT)', 'NOx(GT)', 'NO2(GT)']:\n",
    "    # 1-hour lag\n",
    "    df[f'{pollutant}_lag1'] = df[pollutant].shift(1)\n",
    "    # 24-hour lag (same time yesterday)\n",
    "    df[f'{pollutant}_lag24'] = df[pollutant].shift(24)\n",
    "    # 168-hour lag (same time last week)\n",
    "    df[f'{pollutant}_lag168'] = df[pollutant].shift(168)\n",
    "\n",
    "# Create rolling average features\n",
    "for pollutant in ['CO(GT)', 'NOx(GT)', 'NO2(GT)']:\n",
    "    # 24-hour rolling average\n",
    "    df[f'{pollutant}_rolling24'] = df[pollutant].rolling(window=24).mean()\n",
    "    # 7-day rolling average\n",
    "    df[f'{pollutant}_rolling168'] = df[pollutant].rolling(window=168).mean()\n",
    "\n",
    "# Drop NaN values created by lag and rolling features\n",
    "df_lag = df.dropna()\n",
    "\n",
    "# Visualize correlation between current and lagged values using plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=['CO(GT) vs CO(GT)_lag1', 'CO(GT) vs CO(GT)_lag24', 'CO(GT) vs CO(GT)_lag168'],\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "lag_types = ['lag1', 'lag24', 'lag168']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "for i, lag in enumerate(lag_types):\n",
    "    corr = df_lag['CO(GT)'].corr(df_lag[f'CO(GT)_{lag}']).round(3)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_lag['CO(GT)'],\n",
    "            y=df_lag[f'CO(GT)_{lag}'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=colors[i],\n",
    "                opacity=0.5,\n",
    "                size=5\n",
    "            ),\n",
    "            name=f'Lag {lag.replace(\"lag\", \"\")}',\n",
    "            hovertemplate='Current: %{x:.2f}<br>Lag: %{y:.2f}<extra></extra>'\n",
    "        ),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "    \n",
    "    # Add correlation annotation\n",
    "    fig.add_annotation(\n",
    "        x=0.05,\n",
    "        y=0.95,\n",
    "        text=f'r = {corr}',\n",
    "        xref=f'x{i+1}',\n",
    "        yref=f'y{i+1}',\n",
    "        showarrow=False,\n",
    "        font=dict(size=12),\n",
    "        bgcolor='white',\n",
    "        bordercolor='black',\n",
    "        borderwidth=1,\n",
    "        borderpad=3\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    width=900,\n",
    "    title_text='Correlation Between Current and Lagged CO(GT) Values',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "for i in range(3):\n",
    "    fig.update_xaxes(title_text='CO(GT) Current', row=1, col=i+1)\n",
    "    fig.update_yaxes(title_text=f'CO(GT) {lag_types[i]}', row=1, col=i+1)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/preprocessing_plots/correlation_plotly.png', scale=1.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for Lag and Rolling Features\n",
    "\n",
    "Lag and rolling features are particularly valuable for time series data:\n",
    "\n",
    "1. **Temporal Dependency**: They capture the autocorrelation in pollution levels over time.\n",
    "2. **Predictive Power**: Previous pollution levels are often strong predictors of current levels.\n",
    "3. **Trend Capture**: Rolling averages help capture longer-term trends and smooth out noise.\n",
    "4. **Cyclical Patterns**: Lag features at specific intervals (24 hours, 168 hours) capture daily and weekly patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Features\n",
    "\n",
    "Creating interaction features to capture relationships between different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features between temperature and pollutants\n",
    "for pollutant in ['CO(GT)', 'NOx(GT)', 'NO2(GT)']:\n",
    "    df[f'{pollutant}_T_interaction'] = df[pollutant] * df['T']\n",
    "\n",
    "# Create interaction features between humidity and pollutants\n",
    "for pollutant in ['CO(GT)', 'NOx(GT)', 'NO2(GT)']:\n",
    "    df[f'{pollutant}_RH_interaction'] = df[pollutant] * df['RH']\n",
    "\n",
    "# Create interaction between rush hour and weekend\n",
    "df['rush_hour_weekend'] = df['is_rush_hour'] * df['is_weekend']\n",
    "\n",
    "# Visualize one of the interaction features using plotly\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(\n",
    "    df, \n",
    "    x='T', \n",
    "    y='CO(GT)', \n",
    "    color='CO(GT)_T_interaction',\n",
    "    color_continuous_scale='viridis',\n",
    "    opacity=0.6,\n",
    "    title='Temperature vs CO(GT) Colored by Their Interaction',\n",
    "    labels={'T': 'Temperature', 'CO(GT)': 'CO(GT) Concentration', 'CO(GT)_T_interaction': 'CO(GT) × Temperature Interaction'},\n",
    "    height=600,\n",
    "    width=800\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    coloraxis_colorbar=dict(title='CO(GT) × T'),\n",
    "    hovermode='closest',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/preprocessing_plots/interaction_plotly.png', scale=1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for Interaction Features\n",
    "\n",
    "Interaction features capture complex relationships between variables:\n",
    "\n",
    "1. **Non-linear Relationships**: They help models capture non-linear relationships between variables.\n",
    "2. **Environmental Chemistry**: Pollutant behavior often depends on interactions with environmental factors like temperature and humidity.\n",
    "3. **Compound Effects**: Some effects only manifest when multiple factors coincide (e.g., rush hour on weekdays vs. weekends).\n",
    "4. **Model Flexibility**: They give linear models the ability to capture more complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset with engineered features\n",
    "df.to_csv('feature_engineering/data_with_engineered_features.csv')\n",
    "\n",
    "# Create a summary of all engineered features\n",
    "engineered_features = [\n",
    "    'is_rush_hour', 'is_weekend', 'season', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos',\n",
    "    'CO(GT)_lag1', 'CO(GT)_lag24', 'CO(GT)_lag168', 'CO(GT)_rolling24', 'CO(GT)_rolling168',\n",
    "    'NOx(GT)_lag1', 'NOx(GT)_lag24', 'NOx(GT)_lag168', 'NOx(GT)_rolling24', 'NOx(GT)_rolling168',\n",
    "    'NO2(GT)_lag1', 'NO2(GT)_lag24', 'NO2(GT)_lag168', 'NO2(GT)_rolling24', 'NO2(GT)_rolling168',\n",
    "    'CO(GT)_T_interaction', 'NOx(GT)_T_interaction', 'NO2(GT)_T_interaction',\n",
    "    'CO(GT)_RH_interaction', 'NOx(GT)_RH_interaction', 'NO2(GT)_RH_interaction',\n",
    "    'rush_hour_weekend'\n",
    "]\n",
    "\n",
    "print(\"\\nEngineered Features Summary:\")\n",
    "print(f\"Number of original features: {len(airquality_clean.columns)}\")\n",
    "print(f\"Number of engineered features: {len(engineered_features)}\")\n",
    "print(f\"Total number of features: {len(df.columns)}\")\n",
    "print(\"\\nEngineered features saved to 'feature_engineering/data_with_engineered_features.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Summary\n",
    "\n",
    "We have created several categories of engineered features to enhance our analysis and modeling capabilities:\n",
    "\n",
    "1. **Temporal Features**:\n",
    "   - Rush hour indicator\n",
    "   - Weekend/weekday indicator\n",
    "   - Season categorization\n",
    "   - Cyclical time representations (hour, day, month)\n",
    "\n",
    "2. **Lag and Rolling Features**:\n",
    "   - 1-hour, 24-hour, and 168-hour lags for key pollutants\n",
    "   - 24-hour and 7-day rolling averages\n",
    "\n",
    "3. **Interaction Features**:\n",
    "   - Pollutant × Temperature interactions\n",
    "   - Pollutant × Humidity interactions\n",
    "   - Rush hour × Weekend interaction\n",
    "\n",
    "These engineered features capture important temporal patterns, environmental relationships, and complex interactions that can significantly improve model performance and provide deeper insights into air quality dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3 Summary\n",
    "\n",
    "In the data preprocessing and feature engineering phase, we prepared the dataset for advanced analysis and modeling by addressing data quality issues and creating new features to enhance predictive power. This phase built upon the insights gained from the exploratory data analysis to develop a clean, feature-rich dataset for subsequent modeling.\n",
    "\n",
    "Key accomplishments in this phase include:\n",
    "\n",
    "- Handling missing values using appropriate imputation techniques based on the nature and extent of missingness in each variable.\n",
    "- Identifying and treating outliers to minimize their impact on analysis results.\n",
    "- Creating temporal features (hour of day, day of week, month, season) to capture cyclical patterns in pollution levels.\n",
    "- Developing lag features and rolling averages to incorporate temporal dependencies in the data.\n",
    "- Normalizing and scaling variables to ensure comparability and improve model performance.\n",
    "\n",
    "The resulting preprocessed dataset provides a solid foundation for the time series analysis and classification modeling in the following phases. The engineered features capture important temporal patterns and relationships that will enhance our ability to understand and predict air quality variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will:\n",
    "- Calculate and visualize the correlation matrix for all numeric variables.\n",
    "- Identify and discuss significant correlations (strong positive/negative).\n",
    "- Visualize key relationships with scatter plots.\n",
    "- Analyze correlations between pollutants, environmental factors, and sensor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "df = pd.read_csv('preprocessing/preprocessed_data.csv', index_col=0, parse_dates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix Calculation\n",
    "We calculate the correlation matrix for all numeric variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Save correlation matrix to CSV\n",
    "corr_matrix.to_csv('correlation_analysis/correlation_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix Heatmap\n",
    "Visualize the correlation matrix as a heatmap to better understand relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create heatmap visualization of correlation matrix using plotly\n",
    "import plotly.express as px\n",
    "\n",
    "# Create the heatmap with plotly\n",
    "fig = px.imshow(\n",
    "    corr_matrix,\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    "    text_auto='.2f',\n",
    "    aspect='auto',\n",
    "    title='Correlation Matrix Heatmap'\n",
    ")\n",
    "\n",
    "# Update layout for better appearance\n",
    "fig.update_layout(\n",
    "    width=900,\n",
    "    height=800,\n",
    "    xaxis_title=None,\n",
    "    yaxis_title=None,\n",
    "    coloraxis_colorbar=dict(\n",
    "        title='Correlation',\n",
    "        thicknessmode=\"pixels\", thickness=20,\n",
    "        lenmode=\"pixels\", len=300,\n",
    "        ticks=\"outside\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/correlation_analysis_plots/correlation_plotly.png', scale=1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significant Correlations\n",
    "Identify strong positive (r > 0.7) and strong negative (r < -0.7) correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get upper triangle of correlation matrix to avoid duplicates\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find strong positive correlations\n",
    "strong_pos = [(i, j, corr_matrix.loc[i, j]) for i in corr_matrix.index for j in corr_matrix.columns \n",
    "              if corr_matrix.loc[i, j] > 0.7 and i != j]\n",
    "strong_pos.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Find strong negative correlations\n",
    "strong_neg = [(i, j, corr_matrix.loc[i, j]) for i in corr_matrix.index for j in corr_matrix.columns \n",
    "              if corr_matrix.loc[i, j] < -0.7 and i != j]\n",
    "strong_neg.sort(key=lambda x: x[2])\n",
    "\n",
    "print('Strong Positive Correlations (r > 0.7):')\n",
    "if strong_pos:\n",
    "    for i, j, corr in strong_pos:\n",
    "        print(f'{i} and {j}: r = {corr:.4f}')\n",
    "else:\n",
    "    print('No strong positive correlations found (r > 0.7)')\n",
    "\n",
    "print('\\nStrong Negative Correlations (r < -0.7):')\n",
    "if strong_neg:\n",
    "    for i, j, corr in strong_neg:\n",
    "        print(f'{i} and {j}: r = {corr:.4f}')\n",
    "else:\n",
    "    print('No strong negative correlations found (r < -0.7)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots for Top Correlations\n",
    "Visualize the strongest positive and negative correlations with scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs('Plotly_Plots/correlation_analysis_plots', exist_ok=True)\n",
    "\n",
    "# Sort all strong correlations by absolute value\n",
    "all_strong = strong_pos + strong_neg\n",
    "all_strong.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "# Get unique pairs (avoid duplicates like A->B and B->A)\n",
    "seen_pairs = set()\n",
    "unique_pairs = []\n",
    "for var1, var2, corr in all_strong:\n",
    "    pair = tuple(sorted([var1, var2]))\n",
    "    if pair not in seen_pairs:\n",
    "        seen_pairs.add(pair)\n",
    "        unique_pairs.append((var1, var2, corr))\n",
    "\n",
    "# Plot each correlation separately\n",
    "for i, (var1, var2, corr) in enumerate(unique_pairs):\n",
    "    # Create a new figure for each correlation\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Color based on correlation (blue for positive, red for negative)\n",
    "    color = 'blue' if corr > 0 else 'red'\n",
    "    \n",
    "    # Add scatter plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[var1],\n",
    "            y=df[var2],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=color,\n",
    "                opacity=0.6,\n",
    "                size=5\n",
    "            ),\n",
    "            name='Data points',\n",
    "            hovertemplate=f\"{var1}: %{{x:.2f}}<br>{var2}: %{{y:.2f}}<extra></extra>\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add correlation trend line\n",
    "    x_range = [df[var1].min(), df[var1].max()]\n",
    "    \n",
    "    # Calculate trend line correctly based on actual regression\n",
    "    z = np.polyfit(df[var1].dropna(), df[var2].dropna(), 1)\n",
    "    p = np.poly1d(z)\n",
    "    y_pred = p(x_range)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_range,\n",
    "            y=y_pred,\n",
    "            mode='lines',\n",
    "            line=dict(color='black', width=1.5),\n",
    "            name='Trend line',\n",
    "            hoverinfo='skip'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        width=800,\n",
    "        title=f\"Correlation between {var1} and {var2} (r = {corr:.4f})\",\n",
    "        xaxis_title=var1,\n",
    "        yaxis_title=var2,\n",
    "        template='plotly_white',\n",
    "        hovermode='closest'\n",
    "    )\n",
    "    \n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    fig.write_image(f'Plotly_Plots/correlation_analysis_plots/correlation_{var1}_{var2}.png', scale=1.5)\n",
    "\n",
    "# Also create the combined plot\n",
    "n_pairs = len(unique_pairs)\n",
    "n_cols = 3  # Keep 3 columns\n",
    "n_rows = math.ceil(n_pairs / n_cols)  # Calculate required rows\n",
    "\n",
    "# Create subplots for all correlations\n",
    "fig = make_subplots(rows=n_rows, cols=n_cols, \n",
    "                   subplot_titles=[f\"{var1} vs {var2} (r = {corr:.4f})\" \n",
    "                                  for var1, var2, corr in unique_pairs])\n",
    "\n",
    "# Add scatter plots to each subplot\n",
    "for i, (var1, var2, corr) in enumerate(unique_pairs):\n",
    "    row = i // n_cols + 1\n",
    "    col = i % n_cols + 1\n",
    "    \n",
    "    # Color based on correlation (blue for positive, red for negative)\n",
    "    color = 'blue' if corr > 0 else 'red'\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[var1],\n",
    "            y=df[var2],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=color,\n",
    "                opacity=0.6,\n",
    "                size=5\n",
    "            ),\n",
    "            hovertemplate=f\"{var1}: %{{x:.2f}}<br>{var2}: %{{y:.2f}}<extra></extra>\"\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    # Add correlation trend line\n",
    "    x_range = [df[var1].min(), df[var1].max()]\n",
    "    \n",
    "    # Calculate trend line correctly based on actual regression\n",
    "    z = np.polyfit(df[var1].dropna(), df[var2].dropna(), 1)\n",
    "    p = np.poly1d(z)\n",
    "    y_pred = p(x_range)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_range,\n",
    "            y=y_pred,\n",
    "            mode='lines',\n",
    "            line=dict(color='black', width=1.5),\n",
    "            showlegend=False,\n",
    "            hoverinfo='skip'\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    # Set axis titles\n",
    "    fig.update_xaxes(title_text=var1, row=row, col=col)\n",
    "    fig.update_yaxes(title_text=var2, row=row, col=col)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=300 * n_rows,  # Adjust height based on number of rows\n",
    "    width=1000,\n",
    "    title_text=f\"All Strong Correlations (|r| > 0.7)\",\n",
    "    showlegend=False,\n",
    "    hovermode='closest',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/correlation_analysis_plots/all_correlations_combined.png', scale=1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pollutant and Environmental Factor Correlations\n",
    "Analyze correlations between pollutants and environmental factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pollutant and Environmental Factor Correlations using Plotly\n",
    "pollutant_cols = ['CO(GT)', 'C6H6(GT)', 'NOx(GT)', 'NO2(GT)']\n",
    "env_cols = ['T', 'RH', 'AH']\n",
    "\n",
    "# Calculate the correlation\n",
    "subset_corr = df[pollutant_cols + env_cols].corr()\n",
    "\n",
    "# Create heatmap with Plotly\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.imshow(\n",
    "    subset_corr,\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    "    text_auto='.2f',\n",
    "    aspect='auto',\n",
    "    title='Correlation Between Pollutants and Environmental Factors'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=900, \n",
    "    height=600,\n",
    "    xaxis_title=None,\n",
    "    yaxis_title=None,\n",
    "    coloraxis_colorbar=dict(\n",
    "        title='Correlation',\n",
    "        thicknessmode=\"pixels\", \n",
    "        thickness=20,\n",
    "        lenmode=\"pixels\", \n",
    "        len=300\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/correlation_analysis_plots/pollutant_env_factor_correlation_plotly.png', scale=1.5)\n",
    "\n",
    "# Return the specific subset of correlations between pollutants and environmental factors\n",
    "subset_corr.loc[pollutant_cols, env_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor Performance Analysis\n",
    "Analyze the correlation between ground truth pollutant measurements and corresponding sensor readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Analyze the correlation between ground truth pollutant measurements and corresponding sensor readings\n",
    "sensor_pairs = [\n",
    "    ('CO(GT)', 'PT08.S1(CO)'),\n",
    "    ('NOx(GT)', 'PT08.S3(NOx)'),\n",
    "    ('NO2(GT)', 'PT08.S4(NO2)')\n",
    "]\n",
    "\n",
    "# Create subplots - one for each sensor pair\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = make_subplots(rows=len(sensor_pairs), cols=1, \n",
    "                    subplot_titles=[f'Correlation between {gt} and {sensor}' for gt, sensor in sensor_pairs],\n",
    "                    vertical_spacing=0.12)\n",
    "\n",
    "for i, (gt, sensor) in enumerate(sensor_pairs):\n",
    "    corr_val = corr_matrix.loc[gt, sensor]\n",
    "    \n",
    "    # Add scatter plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[gt],\n",
    "            y=df[sensor],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                opacity=0.5,\n",
    "                color='steelblue',\n",
    "                size=5\n",
    "            ),\n",
    "            name=f'{gt} vs {sensor}',\n",
    "            hovertemplate=f'{gt}: %{{x:.2f}}<br>{sensor}: %{{y:.2f}}<extra></extra>'\n",
    "        ),\n",
    "        row=i+1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add correlation annotation\n",
    "    fig.add_annotation(\n",
    "        x=0.95,\n",
    "        y=0.95,\n",
    "        text=f'r = {corr_val:.4f}',\n",
    "        xref=f'x{i+1}',\n",
    "        yref=f'y{i+1}',\n",
    "        showarrow=False,\n",
    "        font=dict(size=14, color='red' if corr_val < 0 else 'green'),\n",
    "        bgcolor='white',\n",
    "        bordercolor='black',\n",
    "        borderwidth=1,\n",
    "        borderpad=4,\n",
    "        align='right'\n",
    "    )\n",
    "    \n",
    "    # Set axis titles\n",
    "    fig.update_xaxes(title_text=gt, row=i+1, col=1)\n",
    "    fig.update_yaxes(title_text=sensor, row=i+1, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=900,\n",
    "    width=800,\n",
    "    title_text='Correlation between Ground Truth Pollutants and Sensor Readings',\n",
    "    showlegend=False,\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plotly_Plots/correlation_analysis_plots/pollutant_sensor_correlation_plotly.png', scale=1.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis Summary\n",
    "- The strongest correlations in the dataset are highlighted above.\n",
    "- Pollutant and environmental factor correlations reveal how weather conditions may influence pollution levels.\n",
    "- Sensor performance analysis shows the relationship between sensor readings and ground truth measurements.\n",
    "- These insights can guide feature selection and further modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation analysis completed. Results saved to correlation_analysis/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase delves into the temporal characteristics of the air quality data. We will perform time series decomposition to identify trend, seasonality, and residuals. Stationarity tests will be conducted, followed by ACF/PACF analysis to inform ARIMA modeling. Finally, an ARIMA model will be developed for forecasting key pollutant concentrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Loading Data for Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the preprocessed data and selecting key pollutants for the time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsa = pd.read_csv('preprocessing/preprocessed_data.csv', index_col='DateTime', parse_dates=True)\n",
    "print(f\"Data for Time Series Analysis loaded. Shape: {df_tsa.shape}\")\n",
    "print(f\"Time range: {df_tsa.index.min()} to {df_tsa.index.max()}\")\n",
    "\n",
    "# Select key pollutants for time series analysis\n",
    "pollutants_tsa = ['CO(GT)', 'NOx(GT)', 'NO2(GT)', 'C6H6(GT)']\n",
    "# Ensure these columns exist\n",
    "pollutants_tsa = [p for p in pollutants_tsa if p in df_tsa.columns]\n",
    "print(f\"Selected pollutants for TSA: {pollutants_tsa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualizing Temporal Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting daily and monthly average concentrations to observe overall trends and seasonal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted from matplotlib/seaborn to Plotly\n",
    "# Resample data to daily averages for better visualization\n",
    "df_daily_tsa = df_tsa[pollutants_tsa].resample('D').mean()\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter(x=df_daily_tsa.index, y=df_daily_tsa[pollutant], name=pollutant) for pollutant in pollutants_tsa])\n",
    "fig.update_layout(title='Daily Average Concentrations of Key Pollutants', yaxis_title='Concentration')\n",
    "fig.write_image(\"Plotly_Plots/time_series_analysis_plots/daily_pollutants_tsa.png\", scale=1.5)\n",
    "fig.show()\n",
    "\n",
    "# Monthly averages for seasonal patterns\n",
    "df_monthly_tsa = df_tsa[pollutants_tsa].resample('M').mean()\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter(x=df_monthly_tsa.index, y=df_monthly_tsa[pollutant], name=pollutant) for pollutant in pollutants_tsa])\n",
    "fig.update_layout(title='Monthly Average Concentrations of Key Pollutants', yaxis_title='Concentration')\n",
    "fig.write_image(\"Plotly_Plots/time_series_analysis_plots/monthly_pollutants_tsa.png\", scale=1.5)\n",
    "fig.show()\n",
    "print(\"Daily and monthly average pollutant concentrations plotted and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing and plotting average hourly and weekly patterns for key pollutants to identify diurnal and weekly cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted from matplotlib/seaborn to Plotly\n",
    "# Hourly patterns (average by hour of day)\n",
    "df_tsa_hourly = df_tsa.copy()\n",
    "df_tsa_hourly['hour'] = df_tsa_hourly.index.hour\n",
    "hourly_patterns_tsa = df_tsa_hourly.groupby('hour')[pollutants_tsa].mean()\n",
    "\n",
    "fig = go.Figure()\n",
    "for i, pollutant in enumerate(pollutants_tsa):\n",
    "    fig.add_trace(go.Scatter(x=hourly_patterns_tsa.index, y=hourly_patterns_tsa[pollutant],\n",
    "                             mode='lines', name=pollutant))\n",
    "fig.update_layout(title='Average Pollutant Concentrations by Hour of Day',\n",
    "                  yaxis_title='Concentration', xaxis_title='Hour of Day')\n",
    "fig.update_xaxes(tickvals=list(range(0, 24, 2)))  # Changed: wrapped range() in list()\n",
    "fig.write_image('time_series_analysis/hourly_patterns_tsa.png', format='png', engine='kaleido') # Requires kaleido package\n",
    "fig.show()\n",
    "\n",
    "# Weekly patterns (average by day of week)\n",
    "df_tsa_weekly = df_tsa.copy()\n",
    "df_tsa_weekly['day_of_week'] = df_tsa_weekly.index.dayofweek # Monday=0, Sunday=6\n",
    "weekly_patterns_tsa = df_tsa_weekly.groupby('day_of_week')[pollutants_tsa].mean()\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "fig = go.Figure()\n",
    "for i, pollutant in enumerate(pollutants_tsa):\n",
    "    fig.add_trace(go.Bar(x=weekly_patterns_tsa.index, y=weekly_patterns_tsa[pollutant],\n",
    "                         name=pollutant))\n",
    "fig.update_layout(title='Average Pollutant Concentrations by Day of Week',\n",
    "                  yaxis_title='Concentration', xaxis_title='Day of Week')\n",
    "fig.update_xaxes(tickvals=list(range(7)), ticktext=days, tickangle=45)  # Changed: wrapped range() in list()\n",
    "fig.write_image('Plotly_Plots/time_series_analysis_plots/weekly_patterns_tsa.png', format='png', engine='kaleido') # Requires kaleido package\n",
    "fig.show()\n",
    "print(\"Hourly and weekly average pollutant patterns plotted and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correct this plotly code and save the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Time Series Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decomposing the CO(GT) time series into trend, seasonal, and residual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted from matplotlib/seaborn to Plotly\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Select CO(GT) for detailed decomposition analysis from daily data\n",
    "target_pollutant_decomp = 'CO(GT)'\n",
    "if target_pollutant_decomp in df_daily_tsa.columns:\n",
    "    ts_decomp = df_daily_tsa[target_pollutant_decomp].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    if not ts_decomp.empty and len(ts_decomp.dropna()) >= 2 * 30: # Ensure enough data for period 30\n",
    "        decomposition = seasonal_decompose(ts_decomp.dropna(), model='additive', period=30)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig = make_subplots(rows=4, cols=1, \n",
    "                           subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residual\"],\n",
    "                           vertical_spacing=0.1)\n",
    "        \n",
    "        # Add traces for each component\n",
    "        fig.add_trace(go.Scatter(x=decomposition.observed.index, y=decomposition.observed, \n",
    "                                mode='lines', name='Observed'), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=decomposition.trend.index, y=decomposition.trend, \n",
    "                                mode='lines', name='Trend'), row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(x=decomposition.seasonal.index, y=decomposition.seasonal, \n",
    "                                mode='lines', name='Seasonal'), row=3, col=1)\n",
    "        fig.add_trace(go.Scatter(x=decomposition.resid.index, y=decomposition.resid, \n",
    "                                mode='lines', name='Residual'), row=4, col=1)\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=800, \n",
    "            width=1000,\n",
    "            title_text=f'Time Series Decomposition of Daily {target_pollutant_decomp}',\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig.write_image(\"Plotly_Plots/time_series_analysis_plots/decomposition_co.png\", format='png', engine='kaleido')  # Requires kaleido package\n",
    "        fig.show()\n",
    "        print(f\"Time series decomposition for {target_pollutant_decomp} completed and saved.\")\n",
    "    else:\n",
    "        print(f\"Not enough data points for decomposition of {target_pollutant_decomp} with period 30 after dropping NaNs. Length: {len(ts_decomp.dropna())}\")\n",
    "else:\n",
    "    print(f\"'{target_pollutant_decomp}' not found in daily resampled data for decomposition.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Stationarity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the Augmented Dickey-Fuller (ADF) test to check for stationarity in the CO(GT) time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "if target_pollutant_decomp in df_daily_tsa.columns and 'ts_decomp' in locals() and not ts_decomp.empty:\n",
    "    ts_adf = ts_decomp.dropna()\n",
    "    if not ts_adf.empty:\n",
    "        print(f\"--- Stationarity Analysis for {target_pollutant_decomp} ---\")\n",
    "        result_adf = adfuller(ts_adf)\n",
    "        print(f'ADF Statistic: {result_adf[0]}')\n",
    "        print(f'p-value: {result_adf[1]}')\n",
    "        print('Critical Values:')\n",
    "        for key, value in result_adf[4].items():\n",
    "            print(f'  {key}: {value}')\n",
    "\n",
    "        if result_adf[1] <= 0.05:\n",
    "            print(f\"Conclusion: The time series for {target_pollutant_decomp} is stationary (reject H0).\")\n",
    "        else:\n",
    "            print(f\"Conclusion: The time series for {target_pollutant_decomp} is not stationary (fail to reject H0). Differencing may be needed.\")\n",
    "    else:\n",
    "        print(f\"Time series for {target_pollutant_decomp} is empty after dropping NaNs for ADF test.\")\n",
    "else:\n",
    "    print(f\"'{target_pollutant_decomp}' or its time series 'ts_decomp' not available for ADF test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Autocorrelation and Partial Autocorrelation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating ACF and PACF plots for the CO(GT) time series to help determine ARIMA model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted from matplotlib/seaborn to Plotly\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "if 'ts_adf' in locals() and not ts_adf.empty:\n",
    "    # Calculate ACF and PACF values\n",
    "    acf_values = acf(ts_adf, nlags=40)\n",
    "    pacf_values = pacf(ts_adf, nlags=40)\n",
    "    \n",
    "    # Create x-axis values (lags)\n",
    "    lags = list(range(len(acf_values)))\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[f'ACF for {target_pollutant_decomp}', \n",
    "                                                        f'PACF for {target_pollutant_decomp}'])\n",
    "    \n",
    "    # Add ACF plot\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=lags, y=acf_values, name='ACF'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add PACF plot\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=lags, y=pacf_values, name='PACF'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Add confidence intervals (approximately 95% CI is ±1.96/sqrt(n))\n",
    "    conf_level = 1.96 / np.sqrt(len(ts_adf))\n",
    "    \n",
    "    # Add confidence intervals to ACF plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=lags, y=[conf_level] * len(lags), mode='lines', line=dict(dash='dash', color='red'), \n",
    "                  showlegend=False),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=lags, y=[-conf_level] * len(lags), mode='lines', line=dict(dash='dash', color='red'), \n",
    "                  showlegend=False),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add confidence intervals to PACF plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=lags, y=[conf_level] * len(lags), mode='lines', line=dict(dash='dash', color='red'), \n",
    "                  showlegend=False),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=lags, y=[-conf_level] * len(lags), mode='lines', line=dict(dash='dash', color='red'), \n",
    "                  showlegend=False),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=500, \n",
    "        width=900,\n",
    "        title_text=f'ACF and PACF for {target_pollutant_decomp}',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text='Lag', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='Lag', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='Correlation', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Partial Correlation', row=1, col=2)\n",
    "    \n",
    "    # Save and show\n",
    "    fig.write_image(\"Plotly_Plots/time_series_analysis_plots/acf_pacf_co.png\", format='png', engine='kaleido')\n",
    "    fig.show()\n",
    "    print(f\"ACF and PACF plots for {target_pollutant_decomp} generated and saved.\")\n",
    "else:\n",
    "    print(f\"Time series 'ts_adf' for {target_pollutant_decomp} not available for ACF/PACF plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 ARIMA Modeling and Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing an ARIMA model to forecast CO(GT) concentrations, evaluating its performance, and generating future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted from matplotlib/seaborn to Plotly\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs('Plotly_Plots/time_series_analysis_plots', exist_ok=True)\n",
    "\n",
    "if 'ts_adf' in locals() and not ts_adf.empty:\n",
    "    ts_arima = ts_adf\n",
    "    train_size = int(len(ts_arima) * 0.8)\n",
    "    train_arima, test_arima = ts_arima[:train_size], ts_arima[train_size:]\n",
    "    print(f\"Training data size: {len(train_arima)}, Test data size: {len(test_arima)}\")\n",
    "\n",
    "    try:\n",
    "        order = (1,1,1) # Example order, may need adjustment based on ACF/PACF and stationarity\n",
    "        print(f\"Attempting ARIMA with order={order}\")\n",
    "        model_arima = ARIMA(train_arima, order=order)\n",
    "        model_fit_arima = model_arima.fit()\n",
    "        print(model_fit_arima.summary())\n",
    "\n",
    "        forecast_steps = len(test_arima)\n",
    "        forecast_arima = model_fit_arima.forecast(steps=forecast_steps)\n",
    "\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=train_arima.index, y=train_arima, mode=\"lines\", name=\"Training Data\"))\n",
    "        fig.add_trace(go.Scatter(x=test_arima.index, y=test_arima, mode=\"lines\", name=\"Actual Test Data\"))\n",
    "        fig.add_trace(go.Scatter(x=test_arima.index, y=forecast_arima, mode=\"lines\", name=\"ARIMA Forecast\", line=dict(color='red')))\n",
    "        fig.update_layout(\n",
    "            title=f'ARIMA {order} Forecast for {target_pollutant_decomp}',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Concentration',\n",
    "            legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01)\n",
    "        )\n",
    "        fig.write_image('Plotly_Plots/time_series_analysis_plots/arima_forecast_tsa.png')\n",
    "        fig.show()\n",
    "\n",
    "        mse_arima = mean_squared_error(test_arima, forecast_arima)\n",
    "        rmse_arima = np.sqrt(mse_arima)\n",
    "        print(f\"ARIMA Model Results for {target_pollutant_decomp}:\")\n",
    "        print(f\"  Mean Squared Error (MSE): {mse_arima:.4f}\")\n",
    "        print(f\"  Root Mean Squared Error (RMSE): {rmse_arima:.4f}\")\n",
    "\n",
    "        future_steps_forecast = 30\n",
    "        full_model_arima = ARIMA(ts_arima, order=order)\n",
    "        full_model_fit_arima = full_model_arima.fit()\n",
    "        future_forecast_values = full_model_fit_arima.forecast(steps=future_steps_forecast)\n",
    "        last_date = ts_arima.index[-1]\n",
    "        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=future_steps_forecast, freq='D')\n",
    "\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=ts_arima.index[-90:], y=ts_arima.iloc[-90:], mode=\"lines\", name=\"Historical Data (Last 90 days)\"))\n",
    "        fig.add_trace(go.Scatter(x=future_dates, y=future_forecast_values, mode=\"lines\", name=f\"{future_steps_forecast}-Day Future Forecast\", line=dict(color='red')))\n",
    "        fig.update_layout(\n",
    "            title=f'{future_steps_forecast}-Day Future Forecast for {target_pollutant_decomp} (ARIMA {order})',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Concentration',\n",
    "            legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01)\n",
    "        )\n",
    "        fig.write_image('Plotly_Plots/time_series_analysis_plots/future_forecast_tsa.png')\n",
    "        fig.show()\n",
    "        print(f\"{future_steps_forecast}-day future forecast generated and saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during ARIMA modeling for {target_pollutant_decomp}: {e}\")\n",
    "else:\n",
    "    print(f\"Time series 'ts_adf' for {target_pollutant_decomp} not available for ARIMA modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5 Summary: Time Series Analysis\n",
    "\n",
    "In this phase, we conducted a comprehensive time series analysis of the air quality data, focusing primarily on CO(GT) concentrations as an example pollutant. The analysis began with visualizing temporal patterns, where daily, monthly, hourly, and weekly trends were plotted. These visualizations helped in understanding the cyclical nature and overall trends in pollutant levels over different time scales.\n",
    "\n",
    "Subsequently, time series decomposition was performed on the daily CO(GT) data. This allowed us to separate the time series into its constituent components: trend, seasonality, and residuals, providing deeper insights into the underlying structure of the data. The trend component showed the long-term direction of CO(GT) levels, while the seasonal component highlighted recurring patterns, and residuals represented the random noise.\n",
    "\n",
    "Stationarity is a key assumption for many time series models. Therefore, the Augmented Dickey-Fuller (ADF) test was employed to check the stationarity of the CO(GT) time series. The results of this test informed whether differencing would be necessary for subsequent modeling. Following the stationarity assessment, Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots were generated. These plots are instrumental in identifying the appropriate orders (p, d, q) for an ARIMA model.\n",
    "\n",
    "Finally, an ARIMA model was developed and fitted to the CO(GT) time series. The model's performance was evaluated by forecasting values for a test period and comparing them against actual observations, using metrics like Mean Squared Error (MSE) and Root Mean Squared Error (RMSE). Additionally, the fitted ARIMA model was used to generate a 30-day forecast beyond the observed data period, providing a projection of future CO(GT) concentrations.\n",
    "\n",
    "Overall, this phase provided valuable insights into the temporal dynamics of air pollution and demonstrated the application of time series modeling techniques for analysis and forecasting. The generated plots and model results are saved in the 'time_series_analysis/' directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6: Advanced Modeling - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase focuses on advanced classification modeling techniques for air quality data analysis. We'll implement various classification models to predict pollution levels based on environmental and temporal features. These techniques provide deeper insights into pollution patterns and enable predictive capabilities for air quality management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Classification Modeling Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll set up the environment for classification modeling and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== CLASSIFICATION MODELING ===\\n\")\n",
    "\n",
    "# Import necessary libraries for classification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Loading Data with Engineered Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset with engineered features for more effective modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with engineered features\n",
    "print(\"Loading data with engineered features...\")\n",
    "try:\n",
    "    df_model = pd.read_csv('feature_engineering/data_with_engineered_features.csv', index_col=0, parse_dates=True)\n",
    "    print(\"Successfully loaded data with engineered features.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Feature-engineered data file not found. Using preprocessed data instead.\")\n",
    "    df_model = pd.read_csv('preprocessing/preprocessed_data.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "print(f\"Loaded dataset with {df_model.shape[0]} rows and {df_model.shape[1]} columns\")\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_model_clean = df_model.dropna()\n",
    "print(f\"Dataset after dropping NaN values: {df_model_clean.shape[0]} rows\")\n",
    "df_model_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Creating Binary Target for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a binary target variable for high pollution events based on CO(GT) concentration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted from matplotlib/seaborn to Plotly\n",
    "# Define target for classification (high pollution vs. normal)\n",
    "target_regression = 'CO(GT)'\n",
    "if target_regression in df_model_clean.columns:\n",
    "    # Using the 75th percentile of CO(GT) as threshold for high pollution\n",
    "    threshold = df_model_clean[target_regression].quantile(0.75)\n",
    "    print(f\"Classification threshold ({target_regression} 75th percentile): {threshold:.4f}\")\n",
    "    \n",
    "    df_model_clean['high_pollution'] = (df_model_clean[target_regression] > threshold).astype(int)\n",
    "    print(f\"Class distribution: {df_model_clean['high_pollution'].value_counts(normalize=True)}\")\n",
    "    \n",
    "    # Visualize the threshold and class distribution\n",
    "    fig = px.histogram(df_model_clean, x=target_regression, nbins=50, opacity=0.7)\n",
    "    fig.add_vline(x=threshold, line_color='red', line_dash='dash', \n",
    "                  annotation_text=f'Threshold (75th percentile): {threshold:.4f}',\n",
    "                  annotation_position=\"top right\")\n",
    "    fig.update_layout(\n",
    "        title=f'Distribution of {target_regression} with High Pollution Threshold',\n",
    "        xaxis_title=f'{target_regression} Concentration',\n",
    "        yaxis_title='Frequency'\n",
    "    )\n",
    "    \n",
    "    # Save figure\n",
    "    fig.write_image('Plotly_Plots/modelling_analysis_plots/high_pollution_threshold.png')\n",
    "    fig.show()\n",
    "else:\n",
    "    print(f\"Target variable {target_regression} not found in dataset. Cannot proceed with classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Feature Selection and Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting relevant features and splitting the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'high_pollution' in df_model_clean.columns:\n",
    "    # Define target and features for classification\n",
    "    target_classification = 'high_pollution'\n",
    "    \n",
    "    # Exclude other ground truth pollutants and the target from features\n",
    "    exclude_cols = ['NOx(GT)', 'NO2(GT)', 'C6H6(GT)', target_regression, target_classification]\n",
    "    feature_cols = [col for col in df_model_clean.columns if col not in exclude_cols \n",
    "                    and df_model_clean[col].dtype in ['float64', 'int64']]\n",
    "    \n",
    "    # Print feature information\n",
    "    print(f\"Number of features: {len(feature_cols)}\")\n",
    "    print(f\"Features: {feature_cols[:5]}... (and {len(feature_cols)-5} more)\")\n",
    "    \n",
    "    # Split the data\n",
    "    X = df_model_clean[feature_cols]\n",
    "    y = df_model_clean[target_classification]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(f\"Training set: {X_train.shape[0]} samples, Test set: {X_test.shape[0]} samples\")\n",
    "    print(f\"Class balance in training set: {pd.Series(y_train).value_counts(normalize=True)}\")\n",
    "    \n",
    "    # Scale the features for models that require it\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "else:\n",
    "    print(\"Target variable 'high_pollution' not found. Cannot proceed with classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a logistic regression model for binary classification of high pollution events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted from matplotlib/seaborn to Plotly\n",
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"\\n1. Logistic Regression\")\n",
    "    \n",
    "    # Create pipeline with standardization\n",
    "    lr_clf_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000))\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    lr_clf_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_lr_clf = lr_clf_pipeline.predict(X_test)\n",
    "    y_prob_lr_clf = lr_clf_pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy_lr = accuracy_score(y_test, y_pred_lr_clf)\n",
    "    print(f\"Logistic Regression Results:\")\n",
    "    print(f\"  Accuracy: {accuracy_lr:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_lr_clf))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm_lr = confusion_matrix(y_test, y_pred_lr_clf)\n",
    "    fig = px.imshow(cm_lr, text_auto=True, color_continuous_scale='Blues',\n",
    "                   x=['Normal', 'High Pollution'], y=['Normal', 'High Pollution'])\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Predicted',\n",
    "        yaxis_title='Actual',\n",
    "        title='Logistic Regression: Confusion Matrix'\n",
    "    )\n",
    "    os.makedirs('Plotly_Plots/modelling_analysis_plots', exist_ok=True)\n",
    "    fig.write_image('Plotly_Plots/modelling_analysis_plots/lr_confusion_matrix.png')\n",
    "    fig.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr_clf)\n",
    "    roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=fpr_lr, y=tpr_lr, mode=\"lines\", \n",
    "                            name=f'Logistic Regression (AUC = {roc_auc_lr:.4f})'))\n",
    "    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\", \n",
    "                            line=dict(dash='dash', color='black'), name='Random'))\n",
    "    fig.update_layout(\n",
    "        xaxis_title='False Positive Rate',\n",
    "        yaxis_title='True Positive Rate',\n",
    "        title='Logistic Regression: ROC Curve',\n",
    "        legend=dict(x=0.7, y=0.1)\n",
    "    )\n",
    "    fig.write_image('Plotly_Plots/modelling_analysis_plots/lr_roc_curve.png')\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Training data not available. Cannot proceed with Logistic Regression.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a Decision Tree classifier with hyperparameter tuning for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted from matplotlib/seaborn to Plotly\n",
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"\\n2. Decision Tree Classifier\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    dt_clf_pipeline = Pipeline([\n",
    "        ('classifier', DecisionTreeClassifier(random_state=42, class_weight='balanced'))\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(dt_clf_pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters and score\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV F1 Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Use the best model\n",
    "    dt_clf_best = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_dt_clf = dt_clf_best.predict(X_test)\n",
    "    y_prob_dt_clf = dt_clf_best.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy_dt = accuracy_score(y_test, y_pred_dt_clf)\n",
    "    print(f\"Decision Tree Classifier Results:\")\n",
    "    print(f\"  Accuracy: {accuracy_dt:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_dt_clf))\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs('Plotly_Plots/modelling_analysis_plots', exist_ok=True)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm_dt = confusion_matrix(y_test, y_pred_dt_clf)\n",
    "    fig = px.imshow(cm_dt, \n",
    "                   text_auto=True, \n",
    "                   color_continuous_scale='Blues',\n",
    "                   x=['Normal', 'High Pollution'],\n",
    "                   y=['Normal', 'High Pollution'])\n",
    "    fig.update_layout(title='Decision Tree: Confusion Matrix',\n",
    "                     xaxis_title='Predicted',\n",
    "                     yaxis_title='Actual')\n",
    "    fig.write_image('Plotly_Plots/modelling_analysis_plots/dt_confusion_matrix.png')\n",
    "    fig.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_dt_clf)\n",
    "    roc_auc_dt = auc(fpr_dt, tpr_dt)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=fpr_dt, y=tpr_dt, mode='lines', \n",
    "                            name=f'Decision Tree (AUC = {roc_auc_dt:.4f})'))\n",
    "    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', \n",
    "                            line=dict(dash='dash', color='black'),\n",
    "                            name='Random'))\n",
    "    fig.update_layout(title='Decision Tree: ROC Curve',\n",
    "                     xaxis_title='False Positive Rate',\n",
    "                     yaxis_title='True Positive Rate',\n",
    "                     showlegend=True)\n",
    "    fig.write_image('Plotly_Plots/modelling_analysis_plots/dt_roc_curve.png')\n",
    "    fig.show()\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(dt_clf_best.named_steps['classifier'], 'feature_importances_'):\n",
    "        dt_features = dt_clf_best.named_steps['classifier'].feature_importances_\n",
    "        feature_importance = pd.DataFrame({'Feature': feature_cols, 'Importance': dt_features})\n",
    "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot feature importance\n",
    "        fig = px.bar(feature_importance.head(15), \n",
    "                    x='Importance', \n",
    "                    y='Feature', \n",
    "                    orientation='h')\n",
    "        fig.update_layout(title='Decision Tree: Top 15 Feature Importance')\n",
    "        fig.write_image('Plotly_Plots/modelling_analysis_plots/dt_feature_importance.png')\n",
    "        fig.show()\n",
    "else:\n",
    "    print(\"Training data not available. Cannot proceed with Decision Tree Classifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a Random Forest classifier with hyperparameter tuning for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted from matplotlib/seaborn to Plotly\n",
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"\\n3. Random Forest Classifier\")\n",
    "    \n",
    "    # Create pipeline with standardization\n",
    "    rf_clf_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100],\n",
    "        'classifier__max_depth': [None, 10, 20]\n",
    "    }\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(rf_clf_pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters and score\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV F1 Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Use the best model\n",
    "    rf_clf_best = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_rf_clf = rf_clf_best.predict(X_test)\n",
    "    y_prob_rf_clf = rf_clf_best.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf_clf)\n",
    "    print(f\"Random Forest Classifier Results:\")\n",
    "    print(f\"  Accuracy: {accuracy_rf:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_rf_clf))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm_rf = confusion_matrix(y_test, y_pred_rf_clf)\n",
    "    fig = px.imshow(cm_rf, \n",
    "                    labels=dict(x=\"Predicted\", y=\"Actual\"),\n",
    "                    x=['Normal', 'High Pollution'],\n",
    "                    y=['Normal', 'High Pollution'],\n",
    "                    text_auto=True,\n",
    "                    color_continuous_scale='Blues')\n",
    "    fig.update_layout(title='Random Forest: Confusion Matrix')\n",
    "    fig.write_image(\"Plotly_Plots/modelling_analysis_plots/rf_confusion_matrix.png\")\n",
    "    fig.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf_clf)\n",
    "    roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=fpr_rf, y=tpr_rf, mode=\"lines\", name=f'Random Forest (AUC = {roc_auc_rf:.4f})'))\n",
    "    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\", name='Baseline', line=dict(dash='dash')))\n",
    "    fig.update_layout(\n",
    "        xaxis_title='False Positive Rate',\n",
    "        yaxis_title='True Positive Rate',\n",
    "        title='Random Forest: ROC Curve',\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig.write_image(\"Plotly_Plots/modelling_analysis_plots/rf_roc_curve.png\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(rf_clf_best.named_steps['classifier'], 'feature_importances_'):\n",
    "        rf_features = rf_clf_best.named_steps['classifier'].feature_importances_\n",
    "        feature_importance = pd.DataFrame({'Feature': feature_cols, 'Importance': rf_features})\n",
    "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot feature importance\n",
    "        fig = px.bar(feature_importance.head(15), \n",
    "                     x='Importance', \n",
    "                     y='Feature', \n",
    "                     orientation='h',\n",
    "                     title='Random Forest: Top 15 Feature Importance')\n",
    "        fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
    "        fig.write_image(\"Plotly_Plots/modelling_analysis_plots/rf_feature_importance.png\")\n",
    "        fig.show()\n",
    "else:\n",
    "    print(\"Training data not available. Cannot proceed with Random Forest Classifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 K-Nearest Neighbors (KNN) Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a K-Nearest Neighbors classifier with hyperparameter tuning for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted from matplotlib/seaborn to Plotly\n",
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"\\n4. K-Nearest Neighbors (KNN) Classifier\")\n",
    "    \n",
    "    # Create pipeline with standardization\n",
    "    knn_clf_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__p': [1, 2]  # p=1 for Manhattan, p=2 for Euclidean\n",
    "    }\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(knn_clf_pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters and score\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV F1 Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Use the best model\n",
    "    knn_clf_best = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_knn_clf = knn_clf_best.predict(X_test)\n",
    "    y_prob_knn_clf = knn_clf_best.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy_knn = accuracy_score(y_test, y_pred_knn_clf)\n",
    "    print(f\"KNN Classifier Results:\")\n",
    "    print(f\"  Accuracy: {accuracy_knn:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_knn_clf))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm_knn = confusion_matrix(y_test, y_pred_knn_clf)\n",
    "    fig = px.imshow(cm_knn, \n",
    "                   labels=dict(x=\"Predicted\", y=\"Actual\"),\n",
    "                   x=['Normal', 'High Pollution'],\n",
    "                   y=['Normal', 'High Pollution'],\n",
    "                   text_auto=True,\n",
    "                   color_continuous_scale='Blues',\n",
    "                   title='KNN: Confusion Matrix')\n",
    "    fig.update_layout(width=800, height=600)\n",
    "    fig.write_image(\"Plotly_Plots/modelling_analysis_plots/knn_confusion_matrix.png\")\n",
    "    fig.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr_knn, tpr_knn, _ = roc_curve(y_test, y_prob_knn_clf)\n",
    "    roc_auc_knn = auc(fpr_knn, tpr_knn)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=fpr_knn, y=tpr_knn, mode=\"lines\", \n",
    "                            name=f'KNN (AUC = {roc_auc_knn:.4f})'))\n",
    "    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\", \n",
    "                            line=dict(dash='dash', color='black'), \n",
    "                            name='Random'))\n",
    "    fig.update_layout(\n",
    "        xaxis_title='False Positive Rate',\n",
    "        yaxis_title='True Positive Rate',\n",
    "        title='KNN: ROC Curve',\n",
    "        width=800, \n",
    "        height=600,\n",
    "        legend=dict(x=0.7, y=0.1)\n",
    "    )\n",
    "    fig.write_image(\"Plotly_Plots/modelling_analysis_plots/knn_roc_curve.png\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Visualize KNN decision boundary (for 2 selected features)\n",
    "    if len(feature_cols) >= 2:\n",
    "        # Select two important features for visualization\n",
    "        if 'feature_importance' in locals() and not feature_importance.empty:\n",
    "            top_features = feature_importance.head(2)['Feature'].values\n",
    "        else:\n",
    "            # If no feature importance available, use first two features\n",
    "            top_features = feature_cols[:2]\n",
    "        \n",
    "        # Train a KNN model on just these two features\n",
    "        X_train_2d = X_train[top_features]\n",
    "        X_test_2d = X_test[top_features]\n",
    "        \n",
    "        # Scale the data\n",
    "        scaler_2d = StandardScaler()\n",
    "        X_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)\n",
    "        X_test_2d_scaled = scaler_2d.transform(X_test_2d)\n",
    "        \n",
    "        # Train KNN with best parameters\n",
    "        best_params = grid_search.best_params_\n",
    "        knn_2d = KNeighborsClassifier(\n",
    "            n_neighbors=best_params['classifier__n_neighbors'],\n",
    "            weights=best_params['classifier__weights'],\n",
    "            p=best_params['classifier__p']\n",
    "        )\n",
    "        knn_2d.fit(X_train_2d_scaled, y_train)\n",
    "        \n",
    "        # Create a mesh grid for decision boundary visualization\n",
    "        h = 0.02  # step size in the mesh\n",
    "        x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1\n",
    "        y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        \n",
    "        # Predict class for each point in the mesh\n",
    "        Z = knn_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        # Create contour plot for decision boundary\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Add contour for decision boundary\n",
    "        fig.add_trace(go.Contour(\n",
    "            z=Z,\n",
    "            x=np.arange(x_min, x_max, h),\n",
    "            y=np.arange(y_min, y_max, h),\n",
    "            colorscale='RdBu',\n",
    "            opacity=0.5,\n",
    "            showscale=False\n",
    "        ))\n",
    "        \n",
    "        # Add scatter plot for training points\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=X_train_2d_scaled[:, 0],\n",
    "            y=X_train_2d_scaled[:, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=y_train,\n",
    "                colorscale='RdBu',\n",
    "                line=dict(color='black', width=1),\n",
    "                size=10,\n",
    "                colorbar=dict(title='Class')\n",
    "            ),\n",
    "            showlegend=False\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'KNN Decision Boundary (n_neighbors={best_params[\"classifier__n_neighbors\"]})',\n",
    "            xaxis_title=f'Scaled {top_features[0]}',\n",
    "            yaxis_title=f'Scaled {top_features[1]}',\n",
    "            width=1000,\n",
    "            height=800\n",
    "        )\n",
    "        \n",
    "        fig.write_image(\"Plotly_Plots/modelling_analysis_plots/knn_decision_boundary.png\")\n",
    "        fig.show()\n",
    "else:\n",
    "    print(\"Training data not available. Cannot proceed with KNN Classifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Support Vector Machine (SVM) Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a Support Vector Machine classifier with hyperparameter tuning for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted from matplotlib/seaborn to Plotly\n",
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"\\n5. Support Vector Machine (SVM) Classifier\")\n",
    "    \n",
    "    # Create pipeline with standardization\n",
    "    svm_clf_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', SVC(random_state=42, probability=True, class_weight='balanced'))\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__kernel': ['linear', 'rbf'],\n",
    "        'classifier__gamma': ['scale', 'auto']\n",
    "    }\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(svm_clf_pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters and score\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV F1 Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Use the best model\n",
    "    svm_clf_best = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_svm_clf = svm_clf_best.predict(X_test)\n",
    "    y_prob_svm_clf = svm_clf_best.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy_svm = accuracy_score(y_test, y_pred_svm_clf)\n",
    "    print(f\"SVM Classifier Results:\")\n",
    "    print(f\"  Accuracy: {accuracy_svm:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_svm_clf))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm_svm = confusion_matrix(y_test, y_pred_svm_clf)\n",
    "    fig = px.imshow(cm_svm, text_auto=True, color_continuous_scale='Blues',\n",
    "                   labels=dict(x=\"Predicted\", y=\"Actual\"),\n",
    "                   x=['Normal', 'High Pollution'],\n",
    "                   y=['Normal', 'High Pollution'])\n",
    "    fig.update_layout(title='SVM: Confusion Matrix')\n",
    "    if not os.path.exists('Plotly_Plots/modelling_analysis_plots'):\n",
    "        os.makedirs('Plotly_Plots/modelling_analysis_plots')\n",
    "    fig.write_image('Plotly_Plots/modelling_analysis_plots/svm_confusion_matrix.png')\n",
    "    fig.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr_svm, tpr_svm, _ = roc_curve(y_test, y_prob_svm_clf)\n",
    "    roc_auc_svm = auc(fpr_svm, tpr_svm)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=fpr_svm, y=tpr_svm, mode='lines', name=f'SVM (AUC = {roc_auc_svm:.4f})'))\n",
    "    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Random', line=dict(dash='dash', color='black')))\n",
    "    fig.update_layout(\n",
    "        xaxis_title='False Positive Rate',\n",
    "        yaxis_title='True Positive Rate',\n",
    "        title='SVM: ROC Curve',\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig.write_image('Plotly_Plots/modelling_analysis_plots/svm_roc_curve.png')\n",
    "    fig.show()\n",
    "    \n",
    "    # Visualize SVM decision boundary (for 2 selected features)\n",
    "    if len(feature_cols) >= 2 and grid_search.best_params_['classifier__kernel'] == 'linear':\n",
    "        # For linear kernel, we can visualize feature coefficients\n",
    "        if hasattr(svm_clf_best.named_steps['classifier'], 'coef_'):\n",
    "            svm_coef = svm_clf_best.named_steps['classifier'].coef_[0]\n",
    "            feature_importance = pd.DataFrame({'Feature': feature_cols, 'Coefficient': np.abs(svm_coef)})\n",
    "            feature_importance = feature_importance.sort_values('Coefficient', ascending=False)\n",
    "            \n",
    "            # Plot feature coefficients\n",
    "            fig = px.bar(feature_importance.head(15), x='Coefficient', y='Feature', orientation='h')\n",
    "            fig.update_layout(title='SVM: Top 15 Feature Coefficients (Absolute Value)')\n",
    "            fig.write_image('Plotly_Plots/modelling_analysis_plots/svm_feature_coefficients.png')\n",
    "            fig.show()\n",
    "            \n",
    "            # Select two important features for visualization\n",
    "            top_features = feature_importance.head(2)['Feature'].values\n",
    "            \n",
    "            # Train an SVM model on just these two features\n",
    "            X_train_2d = X_train[top_features]\n",
    "            X_test_2d = X_test[top_features]\n",
    "            \n",
    "            # Scale the data\n",
    "            scaler_2d = StandardScaler()\n",
    "            X_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)\n",
    "            X_test_2d_scaled = scaler_2d.transform(X_test_2d)\n",
    "            \n",
    "            # Train SVM with best parameters\n",
    "            best_params = grid_search.best_params_\n",
    "            svm_2d = SVC(\n",
    "                C=best_params['classifier__C'],\n",
    "                kernel=best_params['classifier__kernel'],\n",
    "                gamma=best_params['classifier__gamma'],\n",
    "                probability=True,\n",
    "                random_state=42\n",
    "            )\n",
    "            svm_2d.fit(X_train_2d_scaled, y_train)\n",
    "            \n",
    "            # Create a mesh grid for decision boundary visualization\n",
    "            h = 0.02  # step size in the mesh\n",
    "            x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1\n",
    "            y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1\n",
    "            xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "            \n",
    "            # Predict class for each point in the mesh\n",
    "            Z = svm_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            \n",
    "            # Plot the decision boundary\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Add contour for decision boundary\n",
    "            fig.add_trace(go.Contour(\n",
    "                z=Z,\n",
    "                x=np.arange(x_min, x_max, h),\n",
    "                y=np.arange(y_min, y_max, h),\n",
    "                colorscale='RdBu',\n",
    "                opacity=0.5,\n",
    "                showscale=False\n",
    "            ))\n",
    "            \n",
    "            # Plot the training points\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=X_train_2d_scaled[:, 0],\n",
    "                y=X_train_2d_scaled[:, 1],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color=y_train,\n",
    "                    colorscale='RdBu',\n",
    "                    line=dict(width=1)\n",
    "                ),\n",
    "                name='Training Points',\n",
    "                showlegend=True\n",
    "            ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                xaxis_title=f'Scaled {top_features[0]}',\n",
    "                yaxis_title=f'Scaled {top_features[1]}',\n",
    "                title=f'SVM Decision Boundary (C={best_params[\"classifier__C\"]}, kernel={best_params[\"classifier__kernel\"]})'\n",
    "            )\n",
    "            fig.write_image('Plotly_Plots/modelling_analysis_plots/svm_decision_boundary.png')\n",
    "            fig.show()\n",
    "else:\n",
    "    print(\"Training data not available. Cannot proceed with SVM Classifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Recurrent Neural Network (RNN) Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a Recurrent Neural Network (LSTM) classifier for time series classification of pollution events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted from matplotlib/seaborn to Plotly\n",
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"\\n6. Recurrent Neural Network (RNN) Classifier\")\n",
    "    \n",
    "    # For RNN, we need to reshape the data to have a time dimension\n",
    "    # We'll use a simple approach: reshape each sample as a sequence of features\n",
    "    # This is a simplified approach for demonstration purposes\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler_rnn = MinMaxScaler()  # MinMaxScaler works better for neural networks\n",
    "    X_train_scaled_rnn = scaler_rnn.fit_transform(X_train)\n",
    "    X_test_scaled_rnn = scaler_rnn.transform(X_test)\n",
    "    \n",
    "    # Reshape for RNN: [samples, time steps, features]\n",
    "    # We'll treat each feature as a time step for simplicity\n",
    "    n_features = X_train.shape[1]\n",
    "    X_train_rnn = X_train_scaled_rnn.reshape(X_train_scaled_rnn.shape[0], n_features, 1)\n",
    "    X_test_rnn = X_test_scaled_rnn.reshape(X_test_scaled_rnn.shape[0], n_features, 1)\n",
    "    \n",
    "    print(f\"RNN input shape: {X_train_rnn.shape} (samples, time steps, features)\")\n",
    "    \n",
    "    # Build the RNN model\n",
    "    rnn_model = Sequential([\n",
    "        LSTM(64, input_shape=(n_features, 1), return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Print model summary\n",
    "    rnn_model.summary()\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = rnn_model.fit(\n",
    "        X_train_rnn, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[\"RNN: Training and Validation Loss\", \n",
    "                                                       \"RNN: Training and Validation Accuracy\"])\n",
    "    \n",
    "    # Loss subplot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=history.history['loss'], name=\"Training Loss\", mode=\"lines\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=history.history['val_loss'], name=\"Validation Loss\", mode=\"lines\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Accuracy subplot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=history.history['accuracy'], name=\"Training Accuracy\", mode=\"lines\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(y=history.history['val_accuracy'], name=\"Validation Accuracy\", mode=\"lines\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Epoch\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Epoch\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "    \n",
    "    fig.update_layout(height=500, width=1000)\n",
    "    \n",
    "    os.makedirs('Plotly_Plots/modelling_analysis_plots', exist_ok=True)\n",
    "    fig.write_image('Plotly_Plots/modelling_analysis_plots/rnn_training_history.png')\n",
    "    fig.show()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    loss, accuracy = rnn_model.evaluate(X_test_rnn, y_test, verbose=0)\n",
    "    print(f\"RNN Test Loss: {loss:.4f}\")\n",
    "    print(f\"RNN Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba_rnn = rnn_model.predict(X_test_rnn, verbose=0)\n",
    "    y_pred_rnn = (y_pred_proba_rnn > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy_rnn = accuracy_score(y_test, y_pred_rnn)\n",
    "    print(f\"RNN Classifier Results:\")\n",
    "    print(f\"  Accuracy: {accuracy_rnn:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_rnn))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm_rnn = confusion_matrix(y_test, y_pred_rnn)\n",
    "    fig_cm = px.imshow(\n",
    "        cm_rnn, \n",
    "        text_auto=True,\n",
    "        labels=dict(x=\"Predicted\", y=\"Actual\"),\n",
    "        x=['Normal', 'High Pollution'],\n",
    "        y=['Normal', 'High Pollution'],\n",
    "        color_continuous_scale='Blues',\n",
    "        title='RNN: Confusion Matrix'\n",
    "    )\n",
    "    \n",
    "    fig_cm.update_layout(width=700, height=600)\n",
    "    fig_cm.write_image('Plotly_Plots/modelling_analysis_plots/rnn_confusion_matrix.png')\n",
    "    fig_cm.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr_rnn, tpr_rnn, _ = roc_curve(y_test, y_pred_proba_rnn)\n",
    "    roc_auc_rnn = auc(fpr_rnn, tpr_rnn)\n",
    "    \n",
    "    fig_roc = go.Figure()\n",
    "    fig_roc.add_trace(go.Scatter(\n",
    "        x=fpr_rnn, y=tpr_rnn,\n",
    "        name=f'RNN (AUC = {roc_auc_rnn:.4f})',\n",
    "        mode='lines'\n",
    "    ))\n",
    "    fig_roc.add_trace(go.Scatter(\n",
    "        x=[0, 1], y=[0, 1],\n",
    "        name='Baseline',\n",
    "        mode='lines',\n",
    "        line=dict(dash='dash', color='black')\n",
    "    ))\n",
    "    \n",
    "    fig_roc.update_layout(\n",
    "        title='RNN: ROC Curve',\n",
    "        xaxis_title='False Positive Rate',\n",
    "        yaxis_title='True Positive Rate',\n",
    "        width=700, height=600,\n",
    "        legend=dict(x=0.7, y=0.2)\n",
    "    )\n",
    "    \n",
    "    fig_roc.write_image('Plotly_Plots/modelling_analysis_plots/rnn_roc_curve.png')\n",
    "    fig_roc.show()\n",
    "else:\n",
    "    print(\"Training data not available. Cannot proceed with RNN Classifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the performance of all classification models to identify the best approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted from matplotlib/seaborn to Plotly\n",
    "print(\"\\nClassification Model Comparison\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'KNN', 'SVM', 'RNN']\n",
    "accuracy_values = [accuracy_lr, accuracy_dt, accuracy_rf, accuracy_knn, accuracy_svm, accuracy_rnn]\n",
    "auc_values = [roc_auc_lr, roc_auc_dt, roc_auc_rf, roc_auc_knn, roc_auc_svm, roc_auc_rnn]\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Accuracy': accuracy_values,\n",
    "    'AUC': auc_values\n",
    "})\n",
    "\n",
    "print(comparison_df)\n",
    "\n",
    "# Save comparison to CSV\n",
    "comparison_df.to_csv('modelling_analysis_results/classification/model_comparison.csv', index=False)\n",
    "\n",
    "# Plot comparison - create subplots\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Accuracy Comparison\", \"AUC Comparison\"])\n",
    "\n",
    "# Add Accuracy plot\n",
    "fig.add_trace(\n",
    "    go.Bar(x=comparison_df['Model'], y=comparison_df['Accuracy'], name='Accuracy'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add AUC plot\n",
    "fig.add_trace(\n",
    "    go.Bar(x=comparison_df['Model'], y=comparison_df['AUC'], name='AUC'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    width=1000,\n",
    "    showlegend=False,\n",
    "    title_text=\"Model Performance Comparison\"\n",
    ")\n",
    "\n",
    "# Set y-axis range\n",
    "fig.update_yaxes(range=[0.7, 1.0], row=1, col=1)\n",
    "fig.update_yaxes(range=[0.7, 1.0], row=1, col=2)\n",
    "\n",
    "# Update x-axis\n",
    "fig.update_xaxes(tickangle=45, row=1, col=1)\n",
    "fig.update_xaxes(tickangle=45, row=1, col=2)\n",
    "\n",
    "# Save and show\n",
    "fig.write_image('Plotly_Plots/modelling_analysis_plots/model_comparison.png')\n",
    "fig.show()\n",
    "\n",
    "# Combined ROC curves\n",
    "roc_fig = go.Figure()\n",
    "\n",
    "roc_fig.add_trace(go.Scatter(x=fpr_lr, y=tpr_lr, mode=\"lines\", name=f'Logistic Regression (AUC = {roc_auc_lr:.4f})'))\n",
    "roc_fig.add_trace(go.Scatter(x=fpr_dt, y=tpr_dt, mode=\"lines\", name=f'Decision Tree (AUC = {roc_auc_dt:.4f})'))\n",
    "roc_fig.add_trace(go.Scatter(x=fpr_rf, y=tpr_rf, mode=\"lines\", name=f'Random Forest (AUC = {roc_auc_rf:.4f})'))\n",
    "roc_fig.add_trace(go.Scatter(x=fpr_knn, y=tpr_knn, mode=\"lines\", name=f'KNN (AUC = {roc_auc_knn:.4f})'))\n",
    "roc_fig.add_trace(go.Scatter(x=fpr_svm, y=tpr_svm, mode=\"lines\", name=f'SVM (AUC = {roc_auc_svm:.4f})'))\n",
    "roc_fig.add_trace(go.Scatter(x=fpr_rnn, y=tpr_rnn, mode=\"lines\", name=f'RNN (AUC = {roc_auc_rnn:.4f})'))\n",
    "roc_fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\", line=dict(dash='dash', color='black'), name='Random'))\n",
    "\n",
    "roc_fig.update_layout(\n",
    "    title='ROC Curves for All Classification Models',\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    legend=dict(x=0.7, y=0.1),\n",
    "    height=600,\n",
    "    width=800\n",
    ")\n",
    "\n",
    "# Save and show\n",
    "roc_fig.write_image('Plotly_Plots/modelling_analysis_plots/combined_roc_curves.png')\n",
    "roc_fig.show()\n",
    "\n",
    "# Identify the best model\n",
    "best_model_idx = np.argmax(auc_values)\n",
    "best_model = models[best_model_idx]\n",
    "best_auc = auc_values[best_model_idx]\n",
    "best_accuracy = accuracy_values[best_model_idx]\n",
    "\n",
    "print(f\"\\nBest performing model based on AUC: {best_model}\")\n",
    "print(f\"  AUC: {best_auc:.4f}\")\n",
    "print(f\"  Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6 Summary: Advanced Modeling - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Phase 6, we applied advanced classification modeling techniques to predict high pollution events based on environmental and temporal features. We defined a binary target variable by setting a threshold at the 75th percentile of CO(GT) concentrations, effectively distinguishing between normal and high pollution conditions.\n",
    "\n",
    "Six different classification models were implemented and compared: Logistic Regression, Decision Tree, Random Forest, K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Recurrent Neural Network (RNN). Each model was carefully tuned using grid search with cross-validation to optimize hyperparameters and maximize performance.\n",
    "\n",
    "The models were evaluated using multiple metrics including accuracy, precision, recall, F1-score, and AUC-ROC. Confusion matrices were generated to visualize true positives, false positives, true negatives, and false negatives for each model. ROC curves illustrated the trade-off between sensitivity and specificity across different classification thresholds.\n",
    "\n",
    "For tree-based models (Decision Tree and Random Forest), feature importance analysis revealed which variables were most influential in predicting high pollution events. For KNN and SVM, decision boundaries were visualized to provide insight into how these models classify the data in feature space. The RNN model demonstrated how deep learning approaches can capture complex patterns in the data, particularly temporal dependencies.\n",
    "\n",
    "The comprehensive model comparison showed that [best model name] achieved the highest performance with an AUC of [best AUC value] and accuracy of [best accuracy value]. This suggests that [insights about model performance and characteristics].\n",
    "\n",
    "These classification models provide valuable predictive capabilities for air quality management, enabling the forecasting of high pollution events based on measurable environmental and temporal factors. Such predictions can inform public health advisories, traffic management decisions, and other interventions aimed at reducing exposure to harmful air pollutants."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
