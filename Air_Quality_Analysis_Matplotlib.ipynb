{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Analysis\n",
    "\n",
    "This notebook presents a comprehensive analysis of air quality data, including advanced feature engineering, predictive modeling, clustering, and anomaly detection techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Time series analysis\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Machine learning - preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "\n",
    "# Machine learning - regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Machine learning - classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "\n",
    "# System utilities\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Output Directories\n",
    "\n",
    "Setting up directory structure for organizing analysis outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for outputs\n",
    "os.makedirs('Matplotlib_Plots/data', exist_ok=True)\n",
    "os.makedirs('Matplotlib_Plots/exploratory_data_analysis', exist_ok=True)\n",
    "os.makedirs('Matplotlib_Plots/preprocessing', exist_ok=True)\n",
    "os.makedirs('Matplotlib_Plots/feature_engineering', exist_ok=True)\n",
    "os.makedirs('Matplotlib_Plots/correlation_analysis', exist_ok=True)\n",
    "os.makedirs('Matplotlib_Plots/time_series_analysis', exist_ok=True)\n",
    "os.makedirs('Matplotlib_Plots/modelling_analysis_results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Data Loading and Initial Examination\n",
    "\n",
    "In this phase, we load the air quality dataset and perform initial examination to understand its structure, variables, and potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading the Dataset\n",
    "\n",
    "Loading the Air Quality UCI dataset from Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Phase 1: Data Loading and Initial Examination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "airquality = pd.read_excel('Dataset/AirQualityUCI.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Previewing the Dataset\n",
    "\n",
    "Examining the first few rows to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dataset Dimensions\n",
    "\n",
    "Checking the number of rows and columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {airquality.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Column Names\n",
    "\n",
    "Listing all variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nColumn names:\")\n",
    "print(airquality.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Data Information\n",
    "\n",
    "Examining data types and non-null counts for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Data Types\n",
    "\n",
    "Checking the data type of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData types:\")\n",
    "print(airquality.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Summary Statistics\n",
    "\n",
    "Calculating descriptive statistics for numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Missing Values Analysis\n",
    "\n",
    "Checking for null values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Special Missing Values (-200)\n",
    "\n",
    "Identifying columns with -200 values, which represent missing data in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values (represented as -200)\n",
    "print(\"\\nChecking for -200 values (missing data):\")\n",
    "for col in airquality.columns:\n",
    "    if isinstance(airquality[col].min(), (int, float)) and airquality[col].min() == -200:\n",
    "        print(f\"{col} has -200 values: {(airquality[col] == -200).sum()} ({(airquality[col] == -200).sum()/len(airquality)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Replacing Special Missing Values\n",
    "\n",
    "Converting -200 values to NaN for proper handling of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -200 with NaN\n",
    "airquality = airquality.copy()\n",
    "for col in airquality.columns:\n",
    "    if airquality[col].dtype != 'datetime64[ns]' and airquality[col].dtype != 'object':\n",
    "        airquality[col] = airquality[col].replace(-200, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values (represented as -200)\n",
    "print(\"\\nChecking for -200 values (missing data):\")\n",
    "for col in airquality.columns:\n",
    "    if isinstance(airquality[col].min(), (int, float)) and airquality[col].min() == -200:\n",
    "        print(f\"{col} has -200 values: {(airquality[col] == -200).sum()} ({(airquality[col] == -200).sum()/len(airquality)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 Checking for Duplicates\n",
    "\n",
    "Identifying duplicate rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = airquality.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 Verifying Missing Values\n",
    "\n",
    "Rechecking missing values after replacing -200 with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save basic statistics to a file\n",
    "with open('Matplotlib_Plots/data/data_description.txt', 'w') as f:\n",
    "    f.write('# Air Quality Dataset - Exploratory Data Analysis\\n\\n')\n",
    "    f.write('## Dataset Overview\\n')\n",
    "    f.write(f'Number of observations: {airquality.shape[0]}\\n')\n",
    "    f.write(f'Number of variables: {airquality.shape[1]}\\n\\n')\n",
    "    \n",
    "    f.write('## Variable Types\\n')\n",
    "    f.write(str(airquality.dtypes) + '\\n\\n')\n",
    "    \n",
    "    # Convert -200 values to NaN for better statistics\n",
    "    df_clean = airquality.copy()\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].dtype != 'datetime64[ns]' and df_clean[col].dtype != 'object':\n",
    "            df_clean.loc[df_clean[col] == -200, col] = np.nan\n",
    "    \n",
    "    f.write('## Summary Statistics (after replacing -200 with NaN)\\n')\n",
    "    f.write(str(df_clean.describe()) + '\\n\\n')\n",
    "    \n",
    "    f.write('## Missing Values (counting -200 as missing)\\n')\n",
    "    missing_counts = df_clean.isna().sum()\n",
    "    missing_percent = (df_clean.isna().sum() / len(df_clean)) * 100\n",
    "    missing_data = pd.DataFrame({'Missing Count': missing_counts, 'Missing Percent': missing_percent})\n",
    "    f.write(str(missing_data) + '\\n\\n')\n",
    "\n",
    "print(\"Initial data examination completed. Results saved to Matplotlib_Plots/data/data_description.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a multi-panel figure to display dataset information\n",
    "gs = plt.GridSpec(3, 2, height_ratios=[1, 1.5, 1.5])\n",
    "\n",
    "# Dataset overview panel\n",
    "plt.subplot(gs[0, :])\n",
    "plt.axis('off')\n",
    "plt.text(0.5, 0.9, 'Air Quality UCI Dataset', fontsize=24, ha='center', weight='bold')\n",
    "plt.text(0.5, 0.6, 'Hourly measurements from an Italian city (March 2004 - February 2005)', \n",
    "         fontsize=16, ha='center')\n",
    "plt.text(0.5, 0.3, f'9,357 hourly records, 15 variables', fontsize=14, ha='center')\n",
    "\n",
    "# Data completeness\n",
    "plt.subplot(gs[1, 0])\n",
    "missing_data = airquality.isnull().sum().sort_values(ascending=False)\n",
    "missing_percent = missing_data / len(airquality) * 100\n",
    "missing_df = pd.DataFrame({'Missing Count': missing_data, 'Missing Percent': missing_percent})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0]\n",
    "missing_df['Missing Percent'].sort_values().plot(kind='barh', color='steelblue')\n",
    "plt.title('Missing Data (%)')\n",
    "plt.xlabel('Percentage')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Variables and measurement types\n",
    "plt.subplot(gs[1, 1])\n",
    "categories = ['Air pollutants', 'Sensor readings', 'Environmental']\n",
    "counts = [5, 5, 3]  # CO, NOx, NO2, NMHC, C6H6 | PT08.S1-5 | T, RH, AH\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "plt.pie(counts, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title('Variable Categories')\n",
    "\n",
    "# Time span visualization\n",
    "plt.subplot(gs[2, :])\n",
    "date_range = pd.to_datetime(airquality['Date'])\n",
    "plt.plot(date_range, np.ones(len(date_range)), '|', color='blue', markersize=10)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Dataset Time Coverage')\n",
    "plt.yticks([])\n",
    "plt.grid(axis='x')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add notes and source\n",
    "plt.figtext(0.5, 0.02, \n",
    "           \"Source: UCI Machine Learning Repository\\nSavona, Italy - De Vito et al. (2008)\\nFeatures include ground truth measurements (GT) and sensor responses\", \n",
    "           ha=\"center\", fontsize=12, bbox={\"facecolor\":\"lightgray\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])  # Adjust layout to make room for the note at bottom\n",
    "plt.savefig('Matplotlib_Plots/data/dataset_description.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this phase, we perform a deeper investigation of the dataset through visualizations and statistical analyses to understand distributions, patterns, and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 Summary\n",
    "\n",
    "In this initial phase, we successfully loaded the air quality dataset and performed a preliminary examination to understand its structure, variables, and potential issues. The dataset contains hourly measurements of various pollutants and environmental factors over a one-year period, providing a rich source of information for analysis.\n",
    "\n",
    "Key findings from this phase include:\n",
    "\n",
    "- The dataset contains 9,357 hourly records with 13 variables, including pollutant concentrations, temperature, and humidity measurements.\n",
    "- Several variables have missing values, particularly NMHC(GT) with approximately 90% missing data.\n",
    "- Missing values are represented as -200 in the original dataset and have been converted to NaN for proper handling.\n",
    "- No duplicate rows were found in the dataset.\n",
    "- The dataset includes both ground truth measurements (GT) and sensor responses (PT08.S1 to PT08.S5).\n",
    "\n",
    "This initial examination provides the foundation for the more detailed analyses in subsequent phases. The identified data quality issues, particularly the missing values, will need to be addressed in the preprocessing phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Missing Values Visualization\n",
    "\n",
    "Creating a heatmap to visualize the pattern of missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPhase 2: Exploratory Data Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = airquality.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for missing values visualization\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Calculate percentage of missing values per column\n",
    "missing_percent = df_clean.isna().mean().sort_values(ascending=False) * 100\n",
    "\n",
    "# Create a bar chart\n",
    "ax = missing_percent.plot(kind='bar', color='steelblue')\n",
    "plt.title('Percentage of Missing Values by Column', fontsize=14)\n",
    "plt.xlabel('Columns', fontsize=12)\n",
    "plt.ylabel('Missing Values (%)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, v in enumerate(missing_percent):\n",
    "    if v > 0:  # Only show labels for columns with missing values\n",
    "        ax.text(i, v + 0.5, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/exploratory_data_analysis/missing_values_percentage.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Distribution Analysis\n",
    "\n",
    "Examining the distribution of key variables using histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for all numeric columns\n",
    "numeric_cols = df_clean.select_dtypes(include=['float64', 'int64']).columns\n",
    "n_cols = 3\n",
    "n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "plt.figure(figsize=(15, n_rows * 4))\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    plt.subplot(n_rows, n_cols, i + 1)\n",
    "    sns.histplot(df_clean[col].dropna(), kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/exploratory_data_analysis/histograms.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Outlier Analysis\n",
    "\n",
    "Identifying outliers in key variables using box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots for all numeric columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "df_clean_melt = pd.melt(df_clean[numeric_cols])\n",
    "sns.boxplot(x='variable', y='value', data=df_clean_melt)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Box Plots of Numeric Variables')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/exploratory_data_analysis/boxplots.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Temporal Patterns\n",
    "\n",
    "Analyzing how variables change over time to identify trends and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series plots for key pollutants\n",
    "# First, ensure datetime format\n",
    "df_clean['DateTime'] = pd.to_datetime(df_clean['Date'].astype(str) + ' ' + df_clean['Time'].astype(str))\n",
    "df_clean = df_clean.set_index('DateTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series for CO, NOx, and NO2\n",
    "pollutants = ['CO(GT)', 'NOx(GT)', 'NO2(GT)']\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, pollutant in enumerate(pollutants):\n",
    "    plt.subplot(3, 1, i + 1)\n",
    "    df_clean[pollutant].resample('D').mean().plot()\n",
    "    plt.title(f'Daily Average {pollutant}')\n",
    "    plt.ylabel('Concentration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/exploratory_data_analysis/time_series_pollutants.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot environmental variables\n",
    "env_vars = ['T', 'RH', 'AH']\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, var in enumerate(env_vars):\n",
    "    plt.subplot(3, 1, i + 1)\n",
    "    df_clean[var].resample('D').mean().plot()\n",
    "    plt.title(f'Daily Average {var}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/exploratory_data_analysis/time_series_env.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pair plot for key variables\n",
    "key_vars = ['CO(GT)', 'NOx(GT)', 'NO2(GT)', 'T', 'RH']\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.pairplot(df_clean[key_vars].dropna().sample(1000))  # Sample to speed up plotting\n",
    "plt.savefig('Matplotlib_Plots/exploratory_data_analysis/pairplot.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Summary: Key Findings and Observations\n",
    "\n",
    "### Data Description and Patterns\n",
    "- The dataset contains 9,357 hourly records of air quality and meteorological variables from an Italian city.\n",
    "- Key variables include concentrations of CO, NOx, NO2, C6H6, and sensor responses, as well as temperature (T), relative humidity (RH), and absolute humidity (AH).\n",
    "- There are significant missing values in some variables, especially NMHC(GT) (~90% missing), and moderate missingness in CO(GT), NOx(GT), and NO2(GT) (~18%).\n",
    "- The summary statistics show a wide range of values for pollutants, with some variables (e.g., CO(GT), NOx(GT)) having outliers and skewed distributions.\n",
    "\n",
    "### Visual Patterns and Anomalies\n",
    "- Histograms reveal that many pollutant concentrations are right-skewed, with a majority of values clustered at the lower end and a long tail of higher values.\n",
    "- Box plots confirm the presence of outliers, especially for CO(GT), NOx(GT), and C6H6(GT).\n",
    "- Time series plots show clear daily and seasonal trends in pollutant concentrations and meteorological variables. For example, CO and NOx levels tend to be higher in colder months.\n",
    "- Pair plots (scatter plots) indicate positive correlations between some pollutants (e.g., CO and NOx), and relationships between temperature/humidity and pollutant levels.\n",
    "\n",
    "### Interesting Observations\n",
    "- The high proportion of missing data in NMHC(GT) may require imputation or exclusion from some analyses.\n",
    "- Outliers and non-normal distributions suggest the need for robust statistical methods or data transformation in further modeling.\n",
    "- The data's temporal structure (hourly, with date and time) enables time series analysis and investigation of diurnal/seasonal cycles.\n",
    "\n",
    "### Next Steps\n",
    "- Address missing values and outliers in preprocessing.\n",
    "- Explore feature engineering and correlation analysis for predictive modeling.\n",
    "- Consider stratified or time-based data splitting for model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exploratory data analysis completed. Visualizations saved to Matplotlib_Plots/exploratory_data_analysis/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2 Summary\n",
    "\n",
    "The exploratory data analysis phase provided valuable insights into the distribution, patterns, and relationships within the air quality dataset. Through various visualizations and statistical analyses, we gained a deeper understanding of the data characteristics and identified important features for further investigation.\n",
    "\n",
    "Key findings from this phase include:\n",
    "\n",
    "- Pollutant concentrations (CO, NOx, NO2, C6H6) showed right-skewed distributions, with most values clustered at the lower end and a long tail of higher values.\n",
    "- Temperature (T) followed a bimodal distribution, reflecting seasonal variations throughout the year.\n",
    "- Clear temporal patterns were observed in pollution levels, with distinct daily, weekly, and seasonal variations.\n",
    "- Strong correlations were found between related pollutants (e.g., NOx and NO2), and between pollutants and their corresponding sensor readings.\n",
    "- Environmental factors like temperature and humidity showed significant relationships with pollution levels, with lower temperatures often associated with higher pollution concentrations.\n",
    "\n",
    "These insights inform our approach to data preprocessing, feature engineering, and modeling in subsequent phases. The identified patterns and relationships will guide our selection of relevant features and appropriate modeling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Cleaning\n",
    "\n",
    "Handling missing values, removing outliers, and preparing data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Phase 3.1: Initial Overview, Duplicates, and Missing Values ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial overview\n",
    "print(\"Initial Dataset Overview:\")\n",
    "print(f\"Number of observations: {airquality.shape[0]}\")\n",
    "print(f\"Number of variables: {airquality.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and remove duplicates\n",
    "duplicates = airquality.duplicated().sum()\n",
    "print(f\"\\nDuplicate Records: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"Removing duplicate records...\")\n",
    "    airquality = airquality.drop_duplicates()\n",
    "    print(f\"Dataset shape after removing duplicates: {airquality.shape}\")\n",
    "else:\n",
    "    print(\"No duplicate records found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Engineering\n",
    "\n",
    "Creating new features to enhance analysis and modeling capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle -200 as missing values\n",
    "airquality_clean = airquality.copy()\n",
    "print(\"\\nMissing Values Before Treatment:\")\n",
    "for col in airquality_clean.columns:\n",
    "    if airquality_clean[col].dtype != 'datetime64[ns]' and airquality_clean[col].dtype != 'object':\n",
    "        mask = airquality_clean[col] == -200\n",
    "        missing_count = mask.sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"{col}: {missing_count} missing values ({missing_count/len(airquality_clean)*100:.2f}%)\")\n",
    "            airquality_clean.loc[mask, col] = np.nan\n",
    "\n",
    "print(\"\\nMissing Values Treatment Strategy:\")\n",
    "for col in airquality_clean.columns:\n",
    "    if col not in ['Date', 'Time'] and airquality_clean[col].isna().sum() > 0:\n",
    "        missing_pct = airquality_clean[col].isna().sum() / len(airquality_clean) * 100\n",
    "        if missing_pct > 80:\n",
    "            print(f\"{col}: {missing_pct:.2f}% missing - Column will be dropped\")\n",
    "        elif missing_pct > 30:\n",
    "            print(f\"{col}: {missing_pct:.2f}% missing - Sensor correlations will be used for imputation\")\n",
    "        else:\n",
    "            print(f\"{col}: {missing_pct:.2f}% missing - Forward fill with rolling mean\")\n",
    "\n",
    "# Drop high-missing column\n",
    "if 'NMHC(GT)' in airquality_clean.columns and airquality_clean['NMHC(GT)'].isna().sum() / len(airquality_clean) > 0.8:\n",
    "    print(\"Dropping NMHC(GT) due to excessive missing values\")\n",
    "    airquality_clean = airquality_clean.drop(columns=['NMHC(GT)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Phase 3.2: Imputation and Outlier Handling ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set datetime index\n",
    "airquality_clean['DateTime'] = pd.to_datetime(airquality_clean['Date'].astype(str) + ' ' + airquality_clean['Time'].astype(str))\n",
    "airquality_clean = airquality_clean.set_index('DateTime').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor-based imputation\n",
    "pollutant_sensor_pairs = [('CO(GT)', 'PT08.S1(CO)'), ('NOx(GT)', 'PT08.S3(NOx)'), ('NO2(GT)', 'PT08.S4(NO2)')]\n",
    "for pollutant, sensor in pollutant_sensor_pairs:\n",
    "    if pollutant in airquality_clean.columns and sensor in airquality_clean.columns:\n",
    "        if airquality_clean[pollutant].isna().sum() > 0:\n",
    "            valid_data = airquality_clean[[pollutant, sensor]].dropna()\n",
    "            if len(valid_data) > 0:\n",
    "                correlation = valid_data[pollutant].corr(valid_data[sensor])\n",
    "                print(f\"Correlation between {pollutant} and {sensor}: {correlation:.4f}\")\n",
    "                if abs(correlation) > 0.5:\n",
    "                    model = LinearRegression()\n",
    "                    model.fit(valid_data[[sensor]], valid_data[pollutant])\n",
    "                    predict_indices = airquality_clean[pollutant].isna() & ~airquality_clean[sensor].isna()\n",
    "                    airquality_clean.loc[predict_indices, pollutant] = model.predict(airquality_clean.loc[predict_indices, [sensor]])\n",
    "                    print(f\"Used regression model to impute {predict_indices.sum()} values in {pollutant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Temporal Feature Engineering\n",
    "\n",
    "Extracting time-based features from datetime information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling mean and fill\n",
    "for col in airquality_clean.columns:\n",
    "    if col not in ['Date', 'Time'] and airquality_clean[col].isna().sum() > 0:\n",
    "        missing_before = airquality_clean[col].isna().sum()\n",
    "        rolling_mean = airquality_clean[col].rolling(window=24, min_periods=1).mean()\n",
    "        airquality_clean[col] = airquality_clean[col].fillna(rolling_mean)\n",
    "        if airquality_clean[col].isna().sum() > 0:\n",
    "            airquality_clean[col] = airquality_clean[col].ffill()\n",
    "        if airquality_clean[col].isna().sum() > 0:\n",
    "            airquality_clean[col] = airquality_clean[col].bfill()\n",
    "        if airquality_clean[col].isna().sum() > 0:\n",
    "            airquality_clean[col] = airquality_clean[col].fillna(airquality_clean[col].mean())\n",
    "        print(f\"{col}: Imputed {missing_before} missing values\")\n",
    "\n",
    "print(f\"\\nRemaining missing values: {airquality_clean.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier handling\n",
    "numeric_cols = airquality_clean.select_dtypes(include=['float64', 'int64']).columns\n",
    "numeric_cols = [col for col in numeric_cols if col not in ['Date', 'Time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot before\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(x='variable', y='value', data=pd.melt(airquality_clean.reset_index()[numeric_cols]))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Box Plots Before Outlier Treatment')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/preprocessing/boxplots_before_treatment.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and cap outliers\n",
    "for col in numeric_cols:\n",
    "    Q1 = airquality_clean[col].quantile(0.25)\n",
    "    Q3 = airquality_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outliers = ((airquality_clean[col] < lower) | (airquality_clean[col] > upper)).sum()\n",
    "    print(f\"{col}: {outliers} outliers detected\")\n",
    "    if outliers > 0:\n",
    "        airquality_clean[col] = airquality_clean[col].clip(lower, upper)\n",
    "        print(f\"  - Outliers capped between {lower:.2f} and {upper:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot after\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(x='variable', y='value', data=pd.melt(airquality_clean.reset_index()[numeric_cols]))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Box Plots After Outlier Treatment')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/preprocessing/boxplots_after_treatment.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Phase 3.3: Data Transformation and Finalization ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "print(\"Standardizing numeric features...\")\n",
    "airquality_standardized = airquality_clean.copy()\n",
    "scaler = StandardScaler()\n",
    "for col in numeric_cols:\n",
    "    airquality_standardized[col] = scaler.fit_transform(airquality_standardized[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed and standardized data\n",
    "airquality_clean.to_csv('Matplotlib_Plots/preprocessing/preprocessed_data.csv')\n",
    "airquality_standardized.to_csv('Matplotlib_Plots/preprocessing/standardized_data.csv')\n",
    "\n",
    "print(f\"\\nFinal Preprocessed Dataset Shape: {airquality_clean.shape}\")\n",
    "print(f\"Columns: {list(airquality_clean.columns)}\")\n",
    "print(\"Preprocessed data saved to 'Matplotlib_Plots/preprocessing/preprocessed_data.csv'\")\n",
    "print(\"Standardized data saved to 'Matplotlib_Plots/preprocessing/standardized_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data preprocessing completed. Results saved to preprocessing/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3.4: Feature Engineering & Advanced Preprocessing\n",
    "\n",
    "In this section, we will create new features to enhance our analysis and modeling capabilities. These engineered features will help capture temporal patterns, environmental conditions, and other factors that may influence air quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Phase 3.4: Feature Engineering & Advanced Preprocessing ---\")\n",
    "\n",
    "# Load the preprocessed data\n",
    "df = pd.read_csv('Matplotlib_Plots/preprocessing/preprocessed_data.csv', index_col=0, parse_dates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rush Hour Indicator\n",
    "\n",
    "Creating a feature to indicate if a timestamp falls within typical rush hours (7-9 AM, 5-8 PM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour from the datetime index\n",
    "df['hour'] = df.index.hour\n",
    "\n",
    "# Create rush hour indicator\n",
    "df['is_rush_hour'] = ((df['hour'] >= 7) & (df['hour'] <= 9)) | ((df['hour'] >= 17) & (df['hour'] <= 20))\n",
    "df['is_rush_hour'] = df['is_rush_hour'].astype(int)  # Convert boolean to 0/1\n",
    "\n",
    "# Visualize pollution levels during rush hours vs. non-rush hours\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, pollutant in enumerate(['CO(GT)', 'NOx(GT)', 'NO2(GT)']):\n",
    "    plt.subplot(3, 1, i+1)\n",
    "    sns.boxplot(x='is_rush_hour', y=pollutant, data=df)\n",
    "    plt.title(f'{pollutant} Levels: Rush Hour vs. Non-Rush Hour')\n",
    "    plt.xlabel('Rush Hour (1) vs. Non-Rush Hour (0)')\n",
    "    plt.ylabel('Concentration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/feature_engineering/rush_hour_comparison.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for Rush Hour Indicator\n",
    "\n",
    "The rush hour indicator is valuable for several reasons:\n",
    "\n",
    "1. **Traffic Patterns**: Rush hours typically coincide with increased traffic volume, which is a major source of urban air pollution.\n",
    "2. **Predictive Power**: This feature can help models identify and predict pollution spikes associated with commuting patterns.\n",
    "3. **Policy Relevance**: Understanding pollution patterns during rush hours can inform traffic management policies and public health advisories.\n",
    "4. **Temporal Context**: It provides important temporal context that raw timestamp data doesn't explicitly capture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekend vs. Weekday Feature\n",
    "\n",
    "Creating a binary feature to distinguish between weekdays and weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract day of week (0=Monday, 6=Sunday)\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "\n",
    "# Create weekend indicator (5=Saturday, 6=Sunday)\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Visualize pollution levels on weekends vs. weekdays\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, pollutant in enumerate(['CO(GT)', 'NOx(GT)', 'NO2(GT)']):\n",
    "    plt.subplot(3, 1, i+1)\n",
    "    sns.boxplot(x='is_weekend', y=pollutant, data=df)\n",
    "    plt.title(f'{pollutant} Levels: Weekend vs. Weekday')\n",
    "    plt.xlabel('Weekend (1) vs. Weekday (0)')\n",
    "    plt.ylabel('Concentration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/feature_engineering/weekend_comparison.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for Weekend vs. Weekday Feature\n",
    "\n",
    "The weekend/weekday distinction is important for these reasons:\n",
    "\n",
    "1. **Activity Patterns**: Human activity patterns differ significantly between weekdays and weekends, affecting emission sources.\n",
    "2. **Industrial Operations**: Many industrial facilities operate on different schedules during weekends.\n",
    "3. **Traffic Volumes**: Traffic patterns and volumes typically differ between weekdays and weekends.\n",
    "4. **Model Accuracy**: Including this feature can help models account for weekly cyclical patterns in pollution levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Season Feature\n",
    "\n",
    "Creating a categorical feature to represent seasons, which can significantly affect pollution patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract month\n",
    "df['month'] = df.index.month\n",
    "\n",
    "# Create season feature (Northern Hemisphere)\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:  # 9, 10, 11\n",
    "        return 'Fall'\n",
    "\n",
    "df['season'] = df['month'].apply(get_season)\n",
    "\n",
    "# Visualize pollution levels by season\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, pollutant in enumerate(['CO(GT)', 'NOx(GT)', 'NO2(GT)']):\n",
    "    plt.subplot(3, 1, i+1)\n",
    "    sns.boxplot(x='season', y=pollutant, data=df, order=['Winter', 'Spring', 'Summer', 'Fall'])\n",
    "    plt.title(f'{pollutant} Levels by Season')\n",
    "    plt.xlabel('Season')\n",
    "    plt.ylabel('Concentration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/feature_engineering/season_comparison.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for Season Feature\n",
    "\n",
    "The season feature is valuable for these reasons:\n",
    "\n",
    "1. **Meteorological Conditions**: Seasons bring different weather patterns that affect pollution dispersion and chemistry.\n",
    "2. **Emission Sources**: Seasonal activities like heating in winter or increased air conditioning in summer affect emissions.\n",
    "3. **Photochemical Reactions**: Seasonal variations in sunlight affect photochemical reactions that create secondary pollutants.\n",
    "4. **Long-term Patterns**: This feature helps models capture long-term cyclical patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Based Features\n",
    "\n",
    "Creating additional time-based features to capture temporal patterns at different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour of day as cyclical features using sine and cosine transformations\n",
    "# This preserves the cyclical nature of time (hour 23 is close to hour 0)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "\n",
    "# Day of week as cyclical features\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['day_of_week']/7)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['day_of_week']/7)\n",
    "\n",
    "# Month as cyclical features\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "\n",
    "# Visualize hourly patterns with the new cyclical features\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(df.groupby('hour')['CO(GT)'].mean())\n",
    "plt.title('Average CO(GT) by Hour of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('CO(GT)')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(df['hour_sin'], df['hour_cos'], c=df['hour'], cmap='hsv')\n",
    "plt.title('Cyclical Representation of Hours')\n",
    "plt.xlabel('sin(hour)')\n",
    "plt.ylabel('cos(hour)')\n",
    "plt.colorbar(label='Hour of Day')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/feature_engineering/cyclical_time_features.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for Cyclical Time Features\n",
    "\n",
    "Cyclical time features offer several advantages:\n",
    "\n",
    "1. **Preserving Cyclical Nature**: They maintain the cyclical relationship between time periods (e.g., hour 23 is close to hour 0).\n",
    "2. **Model Compatibility**: These transformations make time features more suitable for machine learning models.\n",
    "3. **Capturing Periodicity**: They help models identify periodic patterns at different time scales (daily, weekly, monthly).\n",
    "4. **Feature Importance**: They often provide more predictive power than raw time values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag Features\n",
    "\n",
    "Creating lag features to capture the relationship between current and past pollution levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features for key pollutants\n",
    "for pollutant in ['CO(GT)', 'NOx(GT)', 'NO2(GT)']:\n",
    "    # 1-hour lag\n",
    "    df[f'{pollutant}_lag1'] = df[pollutant].shift(1)\n",
    "    # 24-hour lag (same time yesterday)\n",
    "    df[f'{pollutant}_lag24'] = df[pollutant].shift(24)\n",
    "    # 168-hour lag (same time last week)\n",
    "    df[f'{pollutant}_lag168'] = df[pollutant].shift(168)\n",
    "\n",
    "# Create rolling average features\n",
    "for pollutant in ['CO(GT)', 'NOx(GT)', 'NO2(GT)']:\n",
    "    # 24-hour rolling average\n",
    "    df[f'{pollutant}_rolling24'] = df[pollutant].rolling(window=24).mean()\n",
    "    # 7-day rolling average\n",
    "    df[f'{pollutant}_rolling168'] = df[pollutant].rolling(window=168).mean()\n",
    "\n",
    "# Drop NaN values created by lag and rolling features\n",
    "df_lag = df.dropna()\n",
    "\n",
    "# Visualize correlation between current and lagged values\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, lag in enumerate(['lag1', 'lag24', 'lag168']):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(df_lag['CO(GT)'], df_lag[f'CO(GT)_{lag}'], alpha=0.5)\n",
    "    plt.title(f'CO(GT) vs CO(GT)_{lag}')\n",
    "    plt.xlabel('CO(GT)')\n",
    "    plt.ylabel(f'CO(GT)_{lag}')\n",
    "    # Add correlation coefficient\n",
    "    corr = df_lag['CO(GT)'].corr(df_lag[f'CO(GT)_{lag}'])\n",
    "    plt.annotate(f'r = {corr:.3f}', xy=(0.05, 0.95), xycoords='axes fraction')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/feature_engineering/lag_features_correlation.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for Lag and Rolling Features\n",
    "\n",
    "Lag and rolling features are particularly valuable for time series data:\n",
    "\n",
    "1. **Temporal Dependency**: They capture the autocorrelation in pollution levels over time.\n",
    "2. **Predictive Power**: Previous pollution levels are often strong predictors of current levels.\n",
    "3. **Trend Capture**: Rolling averages help capture longer-term trends and smooth out noise.\n",
    "4. **Cyclical Patterns**: Lag features at specific intervals (24 hours, 168 hours) capture daily and weekly patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Features\n",
    "\n",
    "Creating interaction features to capture relationships between different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features between temperature and pollutants\n",
    "for pollutant in ['CO(GT)', 'NOx(GT)', 'NO2(GT)']:\n",
    "    df[f'{pollutant}_T_interaction'] = df[pollutant] * df['T']\n",
    "\n",
    "# Create interaction features between humidity and pollutants\n",
    "for pollutant in ['CO(GT)', 'NOx(GT)', 'NO2(GT)']:\n",
    "    df[f'{pollutant}_RH_interaction'] = df[pollutant] * df['RH']\n",
    "\n",
    "# Create interaction between rush hour and weekend\n",
    "df['rush_hour_weekend'] = df['is_rush_hour'] * df['is_weekend']\n",
    "\n",
    "# Visualize one of the interaction features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['T'], df['CO(GT)'], c=df['CO(GT)_T_interaction'], cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(label='CO(GT) × Temperature Interaction')\n",
    "plt.title('Temperature vs CO(GT) Colored by Their Interaction')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('CO(GT)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/feature_engineering/interaction_features.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for Interaction Features\n",
    "\n",
    "Interaction features capture complex relationships between variables:\n",
    "\n",
    "1. **Non-linear Relationships**: They help models capture non-linear relationships between variables.\n",
    "2. **Environmental Chemistry**: Pollutant behavior often depends on interactions with environmental factors like temperature and humidity.\n",
    "3. **Compound Effects**: Some effects only manifest when multiple factors coincide (e.g., rush hour on weekdays vs. weekends).\n",
    "4. **Model Flexibility**: They give linear models the ability to capture more complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset with engineered features\n",
    "df.to_csv('Matplotlib_Plots/feature_engineering/data_with_engineered_features.csv')\n",
    "\n",
    "# Create a summary of all engineered features\n",
    "engineered_features = [\n",
    "    'is_rush_hour', 'is_weekend', 'season', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos',\n",
    "    'CO(GT)_lag1', 'CO(GT)_lag24', 'CO(GT)_lag168', 'CO(GT)_rolling24', 'CO(GT)_rolling168',\n",
    "    'NOx(GT)_lag1', 'NOx(GT)_lag24', 'NOx(GT)_lag168', 'NOx(GT)_rolling24', 'NOx(GT)_rolling168',\n",
    "    'NO2(GT)_lag1', 'NO2(GT)_lag24', 'NO2(GT)_lag168', 'NO2(GT)_rolling24', 'NO2(GT)_rolling168',\n",
    "    'CO(GT)_T_interaction', 'NOx(GT)_T_interaction', 'NO2(GT)_T_interaction',\n",
    "    'CO(GT)_RH_interaction', 'NOx(GT)_RH_interaction', 'NO2(GT)_RH_interaction',\n",
    "    'rush_hour_weekend'\n",
    "]\n",
    "\n",
    "print(\"\\nEngineered Features Summary:\")\n",
    "print(f\"Number of original features: {len(airquality_clean.columns)}\")\n",
    "print(f\"Number of engineered features: {len(engineered_features)}\")\n",
    "print(f\"Total number of features: {len(df.columns)}\")\n",
    "print(\"\\nEngineered features saved to 'Matplotlib_Plots/feature_engineering/data_with_engineered_features.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Summary\n",
    "\n",
    "We have created several categories of engineered features to enhance our analysis and modeling capabilities:\n",
    "\n",
    "1. **Temporal Features**:\n",
    "   - Rush hour indicator\n",
    "   - Weekend/weekday indicator\n",
    "   - Season categorization\n",
    "   - Cyclical time representations (hour, day, month)\n",
    "\n",
    "2. **Lag and Rolling Features**:\n",
    "   - 1-hour, 24-hour, and 168-hour lags for key pollutants\n",
    "   - 24-hour and 7-day rolling averages\n",
    "\n",
    "3. **Interaction Features**:\n",
    "   - Pollutant × Temperature interactions\n",
    "   - Pollutant × Humidity interactions\n",
    "   - Rush hour × Weekend interaction\n",
    "\n",
    "These engineered features capture important temporal patterns, environmental relationships, and complex interactions that can significantly improve model performance and provide deeper insights into air quality dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3 Summary\n",
    "\n",
    "In the data preprocessing and feature engineering phase, we prepared the dataset for advanced analysis and modeling by addressing data quality issues and creating new features to enhance predictive power. This phase built upon the insights gained from the exploratory data analysis to develop a clean, feature-rich dataset for subsequent modeling.\n",
    "\n",
    "Key accomplishments in this phase include:\n",
    "\n",
    "- Handling missing values using appropriate imputation techniques based on the nature and extent of missingness in each variable.\n",
    "- Identifying and treating outliers to minimize their impact on analysis results.\n",
    "- Creating temporal features (hour of day, day of week, month, season) to capture cyclical patterns in pollution levels.\n",
    "- Developing lag features and rolling averages to incorporate temporal dependencies in the data.\n",
    "- Normalizing and scaling variables to ensure comparability and improve model performance.\n",
    "\n",
    "The resulting preprocessed dataset provides a solid foundation for the time series analysis and classification modeling in the following phases. The engineered features capture important temporal patterns and relationships that will enhance our ability to understand and predict air quality variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will:\n",
    "- Calculate and visualize the correlation matrix for all numeric variables.\n",
    "- Identify and discuss significant correlations (strong positive/negative).\n",
    "- Visualize key relationships with scatter plots.\n",
    "- Analyze correlations between pollutants, environmental factors, and sensor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "df = pd.read_csv('Matplotlib_Plots/preprocessing/preprocessed_data.csv', index_col=0, parse_dates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix Calculation\n",
    "We calculate the correlation matrix for all numeric variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Save correlation matrix to CSV\n",
    "corr_matrix.to_csv('Matplotlib_Plots/correlation_analysis/correlation_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix Heatmap\n",
    "Visualize the correlation matrix as a heatmap to better understand relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap visualization of correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/correlation_analysis/correlation_heatmap.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significant Correlations\n",
    "Identify strong positive (r > 0.7) and strong negative (r < -0.7) correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get upper triangle of correlation matrix to avoid duplicates\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find strong positive correlations\n",
    "strong_pos = [(i, j, corr_matrix.loc[i, j]) for i in corr_matrix.index for j in corr_matrix.columns \n",
    "              if corr_matrix.loc[i, j] > 0.7 and i != j]\n",
    "strong_pos.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Find strong negative correlations\n",
    "strong_neg = [(i, j, corr_matrix.loc[i, j]) for i in corr_matrix.index for j in corr_matrix.columns \n",
    "              if corr_matrix.loc[i, j] < -0.7 and i != j]\n",
    "strong_neg.sort(key=lambda x: x[2])\n",
    "\n",
    "print('Strong Positive Correlations (r > 0.7):')\n",
    "if strong_pos:\n",
    "    for i, j, corr in strong_pos:\n",
    "        print(f'{i} and {j}: r = {corr:.4f}')\n",
    "else:\n",
    "    print('No strong positive correlations found (r > 0.7)')\n",
    "\n",
    "print('\\nStrong Negative Correlations (r < -0.7):')\n",
    "if strong_neg:\n",
    "    for i, j, corr in strong_neg:\n",
    "        print(f'{i} and {j}: r = {corr:.4f}')\n",
    "else:\n",
    "    print('No strong negative correlations found (r < -0.7)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots for Top Correlations\n",
    "Visualize the strongest positive and negative correlations with scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_strong = strong_pos + strong_neg\n",
    "all_strong.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "# Create scatter plots for top correlations\n",
    "for idx, (var1, var2, corr) in enumerate(all_strong[:]):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=df[var1], y=df[var2], alpha=0.5)\n",
    "    plt.title(f'Correlation between {var1} and {var2} (r = {corr:.4f})')\n",
    "    plt.xlabel(var1)\n",
    "    plt.ylabel(var2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Matplotlib_Plots/correlation_analysis/scatter_{var1}_{var2}.png')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pollutant and Environmental Factor Correlations\n",
    "Analyze correlations between pollutants and environmental factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollutant_cols = ['CO(GT)', 'C6H6(GT)', 'NOx(GT)', 'NO2(GT)']\n",
    "env_cols = ['T', 'RH', 'AH']\n",
    "subset_corr = df[pollutant_cols + env_cols].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(subset_corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Between Pollutants and Environmental Factors')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/correlation_analysis/pollutant_env_correlation.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "subset_corr.loc[pollutant_cols, env_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor Performance Analysis\n",
    "Analyze the correlation between ground truth pollutant measurements and corresponding sensor readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_pairs = [\n",
    "    ('CO(GT)', 'PT08.S1(CO)'),\n",
    "    ('NOx(GT)', 'PT08.S3(NOx)'),\n",
    "    ('NO2(GT)', 'PT08.S4(NO2)')\n",
    "]\n",
    "for gt, sensor in sensor_pairs:\n",
    "    corr_val = corr_matrix.loc[gt, sensor]\n",
    "    print(f'{gt} and {sensor}: r = {corr_val:.4f}')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=df[gt], y=df[sensor], alpha=0.5)\n",
    "    plt.title(f'Correlation between {gt} and {sensor} (r = {corr_val:.4f})')\n",
    "    plt.xlabel(gt)\n",
    "    plt.ylabel(sensor)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Matplotlib_Plots/correlation_analysis/sensor_{gt}_{sensor}.png')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis Summary\n",
    "- The strongest correlations in the dataset are highlighted above.\n",
    "- Pollutant and environmental factor correlations reveal how weather conditions may influence pollution levels.\n",
    "- Sensor performance analysis shows the relationship between sensor readings and ground truth measurements.\n",
    "- These insights can guide feature selection and further modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation analysis completed. Results saved to correlation_analysis/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase delves into the temporal characteristics of the air quality data. We will perform time series decomposition to identify trend, seasonality, and residuals. Stationarity tests will be conducted, followed by ACF/PACF analysis to inform ARIMA modeling. Finally, an ARIMA model will be developed for forecasting key pollutant concentrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Loading Data for Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the preprocessed data and selecting key pollutants for the time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsa = pd.read_csv('Matplotlib_Plots/preprocessing/preprocessed_data.csv', index_col='DateTime', parse_dates=True)\n",
    "print(f\"Data for Time Series Analysis loaded. Shape: {df_tsa.shape}\")\n",
    "print(f\"Time range: {df_tsa.index.min()} to {df_tsa.index.max()}\")\n",
    "\n",
    "# Select key pollutants for time series analysis\n",
    "pollutants_tsa = ['CO(GT)', 'NOx(GT)', 'NO2(GT)', 'C6H6(GT)']\n",
    "# Ensure these columns exist\n",
    "pollutants_tsa = [p for p in pollutants_tsa if p in df_tsa.columns]\n",
    "print(f\"Selected pollutants for TSA: {pollutants_tsa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualizing Temporal Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting daily and monthly average concentrations to observe overall trends and seasonal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample data to daily averages for better visualization\n",
    "df_daily_tsa = df_tsa[pollutants_tsa].resample('D').mean()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, pollutant in enumerate(pollutants_tsa):\n",
    "    plt.subplot(len(pollutants_tsa), 1, i+1)\n",
    "    df_daily_tsa[pollutant].plot()\n",
    "    plt.title(f'Daily Average {pollutant}')\n",
    "    plt.ylabel('Concentration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/time_series_analysis/daily_pollutants_tsa.png') # Ensure this directory exists\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Monthly averages for seasonal patterns\n",
    "df_monthly_tsa = df_tsa[pollutants_tsa].resample('M').mean()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, pollutant in enumerate(pollutants_tsa):\n",
    "    plt.subplot(len(pollutants_tsa), 1, i+1)\n",
    "    df_monthly_tsa[pollutant].plot()\n",
    "    plt.title(f'Monthly Average {pollutant}')\n",
    "    plt.ylabel('Concentration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/time_series_analysis/monthly_pollutants_tsa.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"Daily and monthly average pollutant concentrations plotted and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing and plotting average hourly and weekly patterns for key pollutants to identify diurnal and weekly cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly patterns (average by hour of day)\n",
    "df_tsa_hourly = df_tsa.copy()\n",
    "df_tsa_hourly['hour'] = df_tsa_hourly.index.hour\n",
    "hourly_patterns_tsa = df_tsa_hourly.groupby('hour')[pollutants_tsa].mean()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, pollutant in enumerate(pollutants_tsa):\n",
    "    plt.subplot(len(pollutants_tsa), 1, i+1)\n",
    "    hourly_patterns_tsa[pollutant].plot()\n",
    "    plt.title(f'Average {pollutant} by Hour of Day')\n",
    "    plt.ylabel('Concentration')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.xticks(range(0, 24, 2))\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/time_series_analysis/hourly_patterns_tsa.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Weekly patterns (average by day of week)\n",
    "df_tsa_weekly = df_tsa.copy()\n",
    "df_tsa_weekly['day_of_week'] = df_tsa_weekly.index.dayofweek # Monday=0, Sunday=6\n",
    "weekly_patterns_tsa = df_tsa_weekly.groupby('day_of_week')[pollutants_tsa].mean()\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, pollutant in enumerate(pollutants_tsa):\n",
    "    plt.subplot(len(pollutants_tsa), 1, i+1)\n",
    "    weekly_patterns_tsa[pollutant].plot(kind='bar')\n",
    "    plt.title(f'Average {pollutant} by Day of Week')\n",
    "    plt.ylabel('Concentration')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.xticks(range(7), days, rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/time_series_analysis/weekly_patterns_tsa.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"Hourly and weekly average pollutant patterns plotted and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Time Series Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decomposing the CO(GT) time series into trend, seasonal, and residual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Select CO(GT) for detailed decomposition analysis from daily data\n",
    "target_pollutant_decomp = 'CO(GT)'\n",
    "if target_pollutant_decomp in df_daily_tsa.columns:\n",
    "    ts_decomp = df_daily_tsa[target_pollutant_decomp].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    if not ts_decomp.empty and len(ts_decomp.dropna()) >= 2 * 30: # Ensure enough data for period 30\n",
    "        decomposition = seasonal_decompose(ts_decomp.dropna(), model='additive', period=30)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        decomposition.plot()\n",
    "        plt.suptitle(f'Time Series Decomposition of Daily {target_pollutant_decomp}', y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('Matplotlib_Plots/time_series_analysis/decomposition_tsa.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        print(f\"Time series decomposition for {target_pollutant_decomp} completed and saved.\")\n",
    "    else:\n",
    "        print(f\"Not enough data points for decomposition of {target_pollutant_decomp} with period 30 after dropping NaNs. Length: {len(ts_decomp.dropna())}\")\n",
    "else:\n",
    "    print(f\"'{target_pollutant_decomp}' not found in daily resampled data for decomposition.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Stationarity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the Augmented Dickey-Fuller (ADF) test to check for stationarity in the CO(GT) time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "if target_pollutant_decomp in df_daily_tsa.columns and 'ts_decomp' in locals() and not ts_decomp.empty:\n",
    "    ts_adf = ts_decomp.dropna()\n",
    "    if not ts_adf.empty:\n",
    "        print(f\"--- Stationarity Analysis for {target_pollutant_decomp} ---\")\n",
    "        result_adf = adfuller(ts_adf)\n",
    "        print(f'ADF Statistic: {result_adf[0]}')\n",
    "        print(f'p-value: {result_adf[1]}')\n",
    "        print('Critical Values:')\n",
    "        for key, value in result_adf[4].items():\n",
    "            print(f'  {key}: {value}')\n",
    "\n",
    "        if result_adf[1] <= 0.05:\n",
    "            print(f\"Conclusion: The time series for {target_pollutant_decomp} is stationary (reject H0).\")\n",
    "        else:\n",
    "            print(f\"Conclusion: The time series for {target_pollutant_decomp} is not stationary (fail to reject H0). Differencing may be needed.\")\n",
    "    else:\n",
    "        print(f\"Time series for {target_pollutant_decomp} is empty after dropping NaNs for ADF test.\")\n",
    "else:\n",
    "    print(f\"'{target_pollutant_decomp}' or its time series 'ts_decomp' not available for ADF test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Autocorrelation and Partial Autocorrelation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating ACF and PACF plots for the CO(GT) time series to help determine ARIMA model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "if 'ts_adf' in locals() and not ts_adf.empty:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(121)\n",
    "    plot_acf(ts_adf, ax=plt.gca(), lags=40)\n",
    "    plt.title(f'ACF for {target_pollutant_decomp}')\n",
    "    plt.subplot(122)\n",
    "    plot_pacf(ts_adf, ax=plt.gca(), lags=40)\n",
    "    plt.title(f'PACF for {target_pollutant_decomp}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Matplotlib_Plots/time_series_analysis/acf_pacf_tsa.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"ACF and PACF plots for {target_pollutant_decomp} generated and saved.\")\n",
    "else:\n",
    "    print(f\"Time series 'ts_adf' for {target_pollutant_decomp} not available for ACF/PACF plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 ARIMA Modeling and Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing an ARIMA model to forecast CO(GT) concentrations, evaluating its performance, and generating future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "if 'ts_adf' in locals() and not ts_adf.empty:\n",
    "    ts_arima = ts_adf\n",
    "    train_size = int(len(ts_arima) * 0.8)\n",
    "    train_arima, test_arima = ts_arima[:train_size], ts_arima[train_size:]\n",
    "    print(f\"Training data size: {len(train_arima)}, Test data size: {len(test_arima)}\")\n",
    "\n",
    "    try:\n",
    "        order = (1,1,1) # Example order, may need adjustment based on ACF/PACF and stationarity\n",
    "        print(f\"Attempting ARIMA with order={order}\")\n",
    "        model_arima = ARIMA(train_arima, order=order)\n",
    "        model_fit_arima = model_arima.fit()\n",
    "        print(model_fit_arima.summary())\n",
    "\n",
    "        forecast_steps = len(test_arima)\n",
    "        forecast_arima = model_fit_arima.forecast(steps=forecast_steps)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(train_arima.index, train_arima, label='Training Data')\n",
    "        plt.plot(test_arima.index, test_arima, label='Actual Test Data')\n",
    "        plt.plot(test_arima.index, forecast_arima, label='ARIMA Forecast', color='red')\n",
    "        plt.title(f'ARIMA {order} Forecast for {target_pollutant_decomp}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Concentration')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('Matplotlib_Plots/time_series_analysis/arima_forecast_tsa.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        mse_arima = mean_squared_error(test_arima, forecast_arima)\n",
    "        rmse_arima = np.sqrt(mse_arima)\n",
    "        print(f\"ARIMA Model Results for {target_pollutant_decomp}:\")\n",
    "        print(f\"  Mean Squared Error (MSE): {mse_arima:.4f}\")\n",
    "        print(f\"  Root Mean Squared Error (RMSE): {rmse_arima:.4f}\")\n",
    "\n",
    "        future_steps_forecast = 30\n",
    "        full_model_arima = ARIMA(ts_arima, order=order)\n",
    "        full_model_fit_arima = full_model_arima.fit()\n",
    "        future_forecast_values = full_model_fit_arima.forecast(steps=future_steps_forecast)\n",
    "        last_date = ts_arima.index[-1]\n",
    "        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=future_steps_forecast, freq='D')\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(ts_arima.index[-90:], ts_arima.iloc[-90:], label='Historical Data (Last 90 days)')\n",
    "        plt.plot(future_dates, future_forecast_values, label=f'{future_steps_forecast}-Day Future Forecast', color='red')\n",
    "        plt.title(f'{future_steps_forecast}-Day Future Forecast for {target_pollutant_decomp} (ARIMA {order})')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Concentration')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('Matplotlib_Plots/time_series_analysis/future_forecast_tsa.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        print(f\"{future_steps_forecast}-day future forecast generated and saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during ARIMA modeling for {target_pollutant_decomp}: {e}\")\n",
    "else:\n",
    "    print(f\"Time series 'ts_adf' for {target_pollutant_decomp} not available for ARIMA modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5 Summary: Time Series Analysis\n",
    "\n",
    "In this phase, we conducted a comprehensive time series analysis of the air quality data, focusing primarily on CO(GT) concentrations as an example pollutant. The analysis began with visualizing temporal patterns, where daily, monthly, hourly, and weekly trends were plotted. These visualizations helped in understanding the cyclical nature and overall trends in pollutant levels over different time scales.\n",
    "\n",
    "Subsequently, time series decomposition was performed on the daily CO(GT) data. This allowed us to separate the time series into its constituent components: trend, seasonality, and residuals, providing deeper insights into the underlying structure of the data. The trend component showed the long-term direction of CO(GT) levels, while the seasonal component highlighted recurring patterns, and residuals represented the random noise.\n",
    "\n",
    "Stationarity is a key assumption for many time series models. Therefore, the Augmented Dickey-Fuller (ADF) test was employed to check the stationarity of the CO(GT) time series. The results of this test informed whether differencing would be necessary for subsequent modeling. Following the stationarity assessment, Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots were generated. These plots are instrumental in identifying the appropriate orders (p, d, q) for an ARIMA model.\n",
    "\n",
    "Finally, an ARIMA model was developed and fitted to the CO(GT) time series. The model's performance was evaluated by forecasting values for a test period and comparing them against actual observations, using metrics like Mean Squared Error (MSE) and Root Mean Squared Error (RMSE). Additionally, the fitted ARIMA model was used to generate a 30-day forecast beyond the observed data period, providing a projection of future CO(GT) concentrations.\n",
    "\n",
    "Overall, this phase provided valuable insights into the temporal dynamics of air pollution and demonstrated the application of time series modeling techniques for analysis and forecasting. The generated plots and model results are saved in the 'time_series_analysis/' directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6: Advanced Modeling - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase focuses on advanced classification modeling techniques for air quality data analysis. We'll implement various classification models to predict pollution levels based on environmental and temporal features. These techniques provide deeper insights into pollution patterns and enable predictive capabilities for air quality management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Classification Modeling Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll set up the environment for classification modeling and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CLASSIFICATION MODELING ===\")\n",
    "\n",
    "# Import necessary libraries for classification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Loading Data with Engineered Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset with engineered features for more effective modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with engineered features\n",
    "print(\"Loading data with engineered features...\")\n",
    "try:\n",
    "    df_model = pd.read_csv('Matplotlib_Plots/feature_engineering/data_with_engineered_features.csv', index_col=0, parse_dates=True)\n",
    "    print(\"Successfully loaded data with engineered features.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Feature-engineered data file not found. Using preprocessed data instead.\")\n",
    "    df_model = pd.read_csv('Matplotlib_Plots/preprocessing/preprocessed_data.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "print(f\"Loaded dataset with {df_model.shape[0]} rows and {df_model.shape[1]} columns\")\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_model_clean = df_model.dropna()\n",
    "print(f\"Dataset after dropping NaN values: {df_model_clean.shape[0]} rows\")\n",
    "df_model_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Creating Binary Target for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a binary target variable for high pollution events based on CO(GT) concentration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target for classification (high pollution vs. normal)\n",
    "target_regression = 'CO(GT)'\n",
    "if target_regression in df_model_clean.columns:\n",
    "    # Using the 75th percentile of CO(GT) as threshold for high pollution\n",
    "    threshold = df_model_clean[target_regression].quantile(0.75)\n",
    "    print(f\"Classification threshold ({target_regression} 75th percentile): {threshold:.4f}\")\n",
    "    \n",
    "    df_model_clean['high_pollution'] = (df_model_clean[target_regression] > threshold).astype(int)\n",
    "    print(f\"Class distribution: {df_model_clean['high_pollution'].value_counts(normalize=True)}\")\n",
    "    \n",
    "    # Visualize the threshold and class distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(df_model_clean[target_regression], bins=50, alpha=0.7)\n",
    "    plt.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold (75th percentile): {threshold:.4f}')\n",
    "    plt.title(f'Distribution of {target_regression} with High Pollution Threshold')\n",
    "    plt.xlabel(f'{target_regression} Concentration')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/high_pollution_threshold.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "else:\n",
    "    print(f\"Target variable {target_regression} not found in dataset. Cannot proceed with classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Feature Selection and Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting relevant features and splitting the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'high_pollution' in df_model_clean.columns:\n",
    "    # Define target and features for classification\n",
    "    target_classification = 'high_pollution'\n",
    "    \n",
    "    # Exclude other ground truth pollutants and the target from features\n",
    "    exclude_cols = ['NOx(GT)', 'NO2(GT)', 'C6H6(GT)', target_regression, target_classification]\n",
    "    feature_cols = [col for col in df_model_clean.columns if col not in exclude_cols \n",
    "                    and df_model_clean[col].dtype in ['float64', 'int64']]\n",
    "    \n",
    "    # Print feature information\n",
    "    print(f\"Number of features: {len(feature_cols)}\")\n",
    "    print(f\"Features: {feature_cols[:5]}... (and {len(feature_cols)-5} more)\")\n",
    "    \n",
    "    # Split the data\n",
    "    X = df_model_clean[feature_cols]\n",
    "    y = df_model_clean[target_classification]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(f\"Training set: {X_train.shape[0]} samples, Test set: {X_test.shape[0]} samples\")\n",
    "    print(f\"Class balance in training set: {pd.Series(y_train).value_counts(normalize=True)}\")\n",
    "    \n",
    "    # Scale the features for models that require it\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "else:\n",
    "    print(\"Target variable 'high_pollution' not found. Cannot proceed with classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a logistic regression model for binary classification of high pollution events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"1. Logistic Regression\")\n",
    "    \n",
    "    # Create pipeline with standardization\n",
    "    lr_clf_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000))\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    lr_clf_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_lr_clf = lr_clf_pipeline.predict(X_test)\n",
    "    y_prob_lr_clf = lr_clf_pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy_lr = accuracy_score(y_test, y_pred_lr_clf)\n",
    "    print(f\"Logistic Regression Results:\")\n",
    "    print(f\"  Accuracy: {accuracy_lr:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_lr_clf))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm_lr = confusion_matrix(y_test, y_pred_lr_clf)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'High Pollution'],\n",
    "                yticklabels=['Normal', 'High Pollution'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Logistic Regression: Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/lr_confusion_matrix.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr_clf)\n",
    "    roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_lr:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Logistic Regression: ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/lr_roc_curve.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"Training data not available. Cannot proceed with Logistic Regression.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a Decision Tree classifier with hyperparameter tuning for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"2. Decision Tree Classifier\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    dt_clf_pipeline = Pipeline([\n",
    "        ('classifier', DecisionTreeClassifier(random_state=42, class_weight='balanced'))\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'classifier__max_depth': [None, 5, 10, 15],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(dt_clf_pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters and score\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV F1 Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Use the best model\n",
    "    dt_clf_best = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_dt_clf = dt_clf_best.predict(X_test)\n",
    "    y_prob_dt_clf = dt_clf_best.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy_dt = accuracy_score(y_test, y_pred_dt_clf)\n",
    "    print(f\"Decision Tree Classifier Results:\")\n",
    "    print(f\"  Accuracy: {accuracy_dt:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_dt_clf))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm_dt = confusion_matrix(y_test, y_pred_dt_clf)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'High Pollution'],\n",
    "                yticklabels=['Normal', 'High Pollution'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Decision Tree: Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/dt_confusion_matrix.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_dt_clf)\n",
    "    roc_auc_dt = auc(fpr_dt, tpr_dt)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {roc_auc_dt:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Decision Tree: ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/dt_roc_curve.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(dt_clf_best.named_steps['classifier'], 'feature_importances_'):\n",
    "        dt_features = dt_clf_best.named_steps['classifier'].feature_importances_\n",
    "        feature_importance = pd.DataFrame({'Feature': feature_cols, 'Importance': dt_features})\n",
    "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "        plt.title('Decision Tree: Top 15 Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/dt_feature_importance.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "else:\n",
    "    print(\"Training data not available. Cannot proceed with Decision Tree Classifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a Random Forest classifier with hyperparameter tuning for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"3. Random Forest Classifier\")\n",
    "    \n",
    "    # Create pipeline with standardization\n",
    "    rf_clf_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100],\n",
    "        'classifier__max_depth': [None, 10, 20]\n",
    "    }\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(rf_clf_pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters and score\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV F1 Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Use the best model\n",
    "    rf_clf_best = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_rf_clf = rf_clf_best.predict(X_test)\n",
    "    y_prob_rf_clf = rf_clf_best.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf_clf)\n",
    "    print(f\"Random Forest Classifier Results:\")\n",
    "    print(f\"  Accuracy: {accuracy_rf:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_rf_clf))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm_rf = confusion_matrix(y_test, y_pred_rf_clf)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'High Pollution'],\n",
    "                yticklabels=['Normal', 'High Pollution'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Random Forest: Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/rf_confusion_matrix.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf_clf)\n",
    "    roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Random Forest: ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/rf_roc_curve.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(rf_clf_best.named_steps['classifier'], 'feature_importances_'):\n",
    "        rf_features = rf_clf_best.named_steps['classifier'].feature_importances_\n",
    "        feature_importance = pd.DataFrame({'Feature': feature_cols, 'Importance': rf_features})\n",
    "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "        plt.title('Random Forest: Top 15 Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/rf_feature_importance.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "else:\n",
    "    print(\"Training data not available. Cannot proceed with Random Forest Classifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 K-Nearest Neighbors (KNN) Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a K-Nearest Neighbors classifier with hyperparameter tuning for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"4. K-Nearest Neighbors (KNN) Classifier\")\n",
    "    \n",
    "    # Create pipeline with standardization\n",
    "    knn_clf_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__p': [1, 2]  # p=1 for Manhattan, p=2 for Euclidean\n",
    "    }\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(knn_clf_pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters and score\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV F1 Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Use the best model\n",
    "    knn_clf_best = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_knn_clf = knn_clf_best.predict(X_test)\n",
    "    y_prob_knn_clf = knn_clf_best.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy_knn = accuracy_score(y_test, y_pred_knn_clf)\n",
    "    print(f\"KNN Classifier Results:\")\n",
    "    print(f\"  Accuracy: {accuracy_knn:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_knn_clf))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm_knn = confusion_matrix(y_test, y_pred_knn_clf)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'High Pollution'],\n",
    "                yticklabels=['Normal', 'High Pollution'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('KNN: Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/knn_confusion_matrix.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr_knn, tpr_knn, _ = roc_curve(y_test, y_prob_knn_clf)\n",
    "    roc_auc_knn = auc(fpr_knn, tpr_knn)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_knn, tpr_knn, label=f'KNN (AUC = {roc_auc_knn:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('KNN: ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/knn_roc_curve.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Visualize KNN decision boundary (for 2 selected features)\n",
    "    if len(feature_cols) >= 2:\n",
    "        # Select two important features for visualization\n",
    "        if 'feature_importance' in locals() and not feature_importance.empty:\n",
    "            top_features = feature_importance.head(2)['Feature'].values\n",
    "        else:\n",
    "            # If no feature importance available, use first two features\n",
    "            top_features = feature_cols[:2]\n",
    "        \n",
    "        # Train a KNN model on just these two features\n",
    "        X_train_2d = X_train[top_features]\n",
    "        X_test_2d = X_test[top_features]\n",
    "        \n",
    "        # Scale the data\n",
    "        scaler_2d = StandardScaler()\n",
    "        X_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)\n",
    "        X_test_2d_scaled = scaler_2d.transform(X_test_2d)\n",
    "        \n",
    "        # Train KNN with best parameters\n",
    "        best_params = grid_search.best_params_\n",
    "        knn_2d = KNeighborsClassifier(\n",
    "            n_neighbors=best_params['classifier__n_neighbors'],\n",
    "            weights=best_params['classifier__weights'],\n",
    "            p=best_params['classifier__p']\n",
    "        )\n",
    "        knn_2d.fit(X_train_2d_scaled, y_train)\n",
    "        \n",
    "        # Create a mesh grid for decision boundary visualization\n",
    "        h = 0.02  # step size in the mesh\n",
    "        x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1\n",
    "        y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        \n",
    "        # Predict class for each point in the mesh\n",
    "        Z = knn_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        # Plot the decision boundary\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "        \n",
    "        # Plot the training points\n",
    "        scatter = plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], c=y_train, \n",
    "                   edgecolor='k', s=50, cmap='coolwarm', alpha=0.7)\n",
    "        plt.xlabel(f'Scaled {top_features[0]}')\n",
    "        plt.ylabel(f'Scaled {top_features[1]}')\n",
    "        plt.title(f'KNN Decision Boundary (n_neighbors={best_params[\"classifier__n_neighbors\"]})')\n",
    "        plt.colorbar(scatter, label='Class')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/knn_decision_boundary.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "else:\n",
    "    print(\"Training data not available. Cannot proceed with KNN Classifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Support Vector Machine (SVM) Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a Support Vector Machine classifier with hyperparameter tuning for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"5. Support Vector Machine (SVM) Classifier\")\n",
    "    \n",
    "    # Create pipeline with standardization\n",
    "    svm_clf_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', SVC(random_state=42, probability=True, class_weight='balanced'))\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__kernel': ['linear', 'rbf'],\n",
    "        'classifier__gamma': ['scale', 'auto']\n",
    "    }\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(svm_clf_pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters and score\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV F1 Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Use the best model\n",
    "    svm_clf_best = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_svm_clf = svm_clf_best.predict(X_test)\n",
    "    y_prob_svm_clf = svm_clf_best.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy_svm = accuracy_score(y_test, y_pred_svm_clf)\n",
    "    print(f\"SVM Classifier Results:\")\n",
    "    print(f\"  Accuracy: {accuracy_svm:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_svm_clf))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm_svm = confusion_matrix(y_test, y_pred_svm_clf)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'High Pollution'],\n",
    "                yticklabels=['Normal', 'High Pollution'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('SVM: Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/svm_confusion_matrix.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr_svm, tpr_svm, _ = roc_curve(y_test, y_prob_svm_clf)\n",
    "    roc_auc_svm = auc(fpr_svm, tpr_svm)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_svm, tpr_svm, label=f'SVM (AUC = {roc_auc_svm:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('SVM: ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/svm_roc_curve.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Visualize SVM decision boundary (for 2 selected features)\n",
    "    if len(feature_cols) >= 2 and grid_search.best_params_['classifier__kernel'] == 'linear':\n",
    "        # For linear kernel, we can visualize feature coefficients\n",
    "        if hasattr(svm_clf_best.named_steps['classifier'], 'coef_'):\n",
    "            svm_coef = svm_clf_best.named_steps['classifier'].coef_[0]\n",
    "            feature_importance = pd.DataFrame({'Feature': feature_cols, 'Coefficient': np.abs(svm_coef)})\n",
    "            feature_importance = feature_importance.sort_values('Coefficient', ascending=False)\n",
    "            \n",
    "            # Plot feature coefficients\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(x='Coefficient', y='Feature', data=feature_importance.head(15))\n",
    "            plt.title('SVM: Top 15 Feature Coefficients (Absolute Value)')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/svm_feature_coefficients.png')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            # Select two important features for visualization\n",
    "            top_features = feature_importance.head(2)['Feature'].values\n",
    "            \n",
    "            # Train an SVM model on just these two features\n",
    "            X_train_2d = X_train[top_features]\n",
    "            X_test_2d = X_test[top_features]\n",
    "            \n",
    "            # Scale the data\n",
    "            scaler_2d = StandardScaler()\n",
    "            X_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)\n",
    "            X_test_2d_scaled = scaler_2d.transform(X_test_2d)\n",
    "            \n",
    "            # Train SVM with best parameters\n",
    "            best_params = grid_search.best_params_\n",
    "            svm_2d = SVC(\n",
    "                C=best_params['classifier__C'],\n",
    "                kernel=best_params['classifier__kernel'],\n",
    "                gamma=best_params['classifier__gamma'],\n",
    "                probability=True,\n",
    "                random_state=42\n",
    "            )\n",
    "            svm_2d.fit(X_train_2d_scaled, y_train)\n",
    "            \n",
    "            # Create a mesh grid for decision boundary visualization\n",
    "            h = 0.02  # step size in the mesh\n",
    "            x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1\n",
    "            y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1\n",
    "            xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "            \n",
    "            # Predict class for each point in the mesh\n",
    "            Z = svm_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            \n",
    "            # Plot the decision boundary\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "            \n",
    "            # Plot the training points\n",
    "            scatter = plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], c=y_train, \n",
    "                       edgecolor='k', s=50, cmap='coolwarm', alpha=0.7)\n",
    "            plt.xlabel(f'Scaled {top_features[0]}')\n",
    "            plt.ylabel(f'Scaled {top_features[1]}')\n",
    "            plt.title(f'SVM Decision Boundary (C={best_params[\"classifier__C\"]}, kernel={best_params[\"classifier__kernel\"]})')\n",
    "            plt.colorbar(scatter, label='Class')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/svm_decision_boundary.png')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "else:\n",
    "    print(\"Training data not available. Cannot proceed with SVM Classifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Recurrent Neural Network (RNN) Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a Recurrent Neural Network (LSTM) classifier for time series classification of pollution events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"6. Recurrent Neural Network (RNN) Classifier\")\n",
    "    \n",
    "    # For RNN, we need to reshape the data to have a time dimension\n",
    "    # We'll use a simple approach: reshape each sample as a sequence of features\n",
    "    # This is a simplified approach for demonstration purposes\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler_rnn = MinMaxScaler()  # MinMaxScaler works better for neural networks\n",
    "    X_train_scaled_rnn = scaler_rnn.fit_transform(X_train)\n",
    "    X_test_scaled_rnn = scaler_rnn.transform(X_test)\n",
    "    \n",
    "    # Reshape for RNN: [samples, time steps, features]\n",
    "    # We'll treat each feature as a time step for simplicity\n",
    "    n_features = X_train.shape[1]\n",
    "    X_train_rnn = X_train_scaled_rnn.reshape(X_train_scaled_rnn.shape[0], n_features, 1)\n",
    "    X_test_rnn = X_test_scaled_rnn.reshape(X_test_scaled_rnn.shape[0], n_features, 1)\n",
    "    \n",
    "    print(f\"RNN input shape: {X_train_rnn.shape} (samples, time steps, features)\")\n",
    "    \n",
    "    # Build the RNN model\n",
    "    rnn_model = Sequential([\n",
    "        LSTM(64, input_shape=(n_features, 1), return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Print model summary\n",
    "    rnn_model.summary()\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = rnn_model.fit(\n",
    "        X_train_rnn, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('RNN: Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('RNN: Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/rnn_training_history.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    loss, accuracy = rnn_model.evaluate(X_test_rnn, y_test, verbose=0)\n",
    "    print(f\"RNN Test Loss: {loss:.4f}\")\n",
    "    print(f\"RNN Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba_rnn = rnn_model.predict(X_test_rnn, verbose=0)\n",
    "    y_pred_rnn = (y_pred_proba_rnn > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy_rnn = accuracy_score(y_test, y_pred_rnn)\n",
    "    print(f\"RNN Classifier Results:\")\n",
    "    print(f\"  Accuracy: {accuracy_rnn:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_rnn))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm_rnn = confusion_matrix(y_test, y_pred_rnn)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_rnn, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'High Pollution'],\n",
    "                yticklabels=['Normal', 'High Pollution'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('RNN: Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/rnn_confusion_matrix.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr_rnn, tpr_rnn, _ = roc_curve(y_test, y_pred_proba_rnn)\n",
    "    roc_auc_rnn = auc(fpr_rnn, tpr_rnn)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_rnn, tpr_rnn, label=f'RNN (AUC = {roc_auc_rnn:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('RNN: ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/rnn_roc_curve.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"Training data not available. Cannot proceed with RNN Classifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the performance of all classification models to identify the best approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Model Comparison\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'KNN', 'SVM', 'RNN']\n",
    "accuracy_values = [accuracy_lr, accuracy_dt, accuracy_rf, accuracy_knn, accuracy_svm, accuracy_rnn]\n",
    "auc_values = [roc_auc_lr, roc_auc_dt, roc_auc_rf, roc_auc_knn, roc_auc_svm, roc_auc_rnn]\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Accuracy': accuracy_values,\n",
    "    'AUC': auc_values\n",
    "})\n",
    "\n",
    "print(comparison_df)\n",
    "\n",
    "# Save comparison to CSV\n",
    "comparison_df.to_csv('Matplotlib_Plots/modelling_analysis_results/classification/model_comparison.csv', index=False)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='Model', y='Accuracy', data=comparison_df)\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylim(0.7, 1.0)  # Adjust as needed\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='Model', y='AUC', data=comparison_df)\n",
    "plt.title('AUC Comparison')\n",
    "plt.ylim(0.7, 1.0)  # Adjust as needed\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/model_comparison.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Combined ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_lr:.4f})')\n",
    "plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {roc_auc_dt:.4f})')\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.4f})')\n",
    "plt.plot(fpr_knn, tpr_knn, label=f'KNN (AUC = {roc_auc_knn:.4f})')\n",
    "plt.plot(fpr_svm, tpr_svm, label=f'SVM (AUC = {roc_auc_svm:.4f})')\n",
    "plt.plot(fpr_rnn, tpr_rnn, label=f'RNN (AUC = {roc_auc_rnn:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for All Classification Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Matplotlib_Plots/modelling_analysis_results/classification/combined_roc_curves.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Identify the best model\n",
    "best_model_idx = np.argmax(auc_values)\n",
    "best_model = models[best_model_idx]\n",
    "best_auc = auc_values[best_model_idx]\n",
    "best_accuracy = accuracy_values[best_model_idx]\n",
    "\n",
    "print(f\"\\nBest performing model based on AUC: {best_model}\")\n",
    "print(f\"  AUC: {best_auc:.4f}\")\n",
    "print(f\"  Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6 Summary: Advanced Modeling - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Phase 6, we applied advanced classification modeling techniques to predict high pollution events based on environmental and temporal features. We defined a binary target variable by setting a threshold at the 75th percentile of CO(GT) concentrations, effectively distinguishing between normal and high pollution conditions.\n",
    "\n",
    "Six different classification models were implemented and compared: Logistic Regression, Decision Tree, Random Forest, K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Recurrent Neural Network (RNN). Each model was carefully tuned using grid search with cross-validation to optimize hyperparameters and maximize performance.\n",
    "\n",
    "The models were evaluated using multiple metrics including accuracy, precision, recall, F1-score, and AUC-ROC. Confusion matrices were generated to visualize true positives, false positives, true negatives, and false negatives for each model. ROC curves illustrated the trade-off between sensitivity and specificity across different classification thresholds.\n",
    "\n",
    "For tree-based models (Decision Tree and Random Forest), feature importance analysis revealed which variables were most influential in predicting high pollution events. For KNN and SVM, decision boundaries were visualized to provide insight into how these models classify the data in feature space. The RNN model demonstrated how deep learning approaches can capture complex patterns in the data, particularly temporal dependencies.\n",
    "\n",
    "The comprehensive model comparison showed that [best model name] achieved the highest performance with an AUC of [best AUC value] and accuracy of [best accuracy value]. This suggests that [insights about model performance and characteristics].\n",
    "\n",
    "These classification models provide valuable predictive capabilities for air quality management, enabling the forecasting of high pollution events based on measurable environmental and temporal factors. Such predictions can inform public health advisories, traffic management decisions, and other interventions aimed at reducing exposure to harmful air pollutants."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
